{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a53346ec85c64af3a3e57de160873b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f373c50c5e0e477fb05352fe1b84db3c",
              "IPY_MODEL_a17d49adfbf84c44baf37669fa4728c8",
              "IPY_MODEL_84cd6ea8c8724f678b1b5a333f32a761"
            ],
            "layout": "IPY_MODEL_8017e9df46f64b12b68d675e159bad10"
          }
        },
        "f373c50c5e0e477fb05352fe1b84db3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_414bb442470a41b994d385ad5d9f2b7b",
            "placeholder": "​",
            "style": "IPY_MODEL_c75d847afa7a4571969b597609f2392c",
            "value": "Downloading builder script: 100%"
          }
        },
        "a17d49adfbf84c44baf37669fa4728c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc1698ef3ee1437b88028a1f72328959",
            "max": 3459,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a81c8c57dff2427fa9a7a4e466bb9acf",
            "value": 3459
          }
        },
        "84cd6ea8c8724f678b1b5a333f32a761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_732c175e72f8480d898c9fa2063d7515",
            "placeholder": "​",
            "style": "IPY_MODEL_e9dbb14ed9dd4c23bac9b14fc2972434",
            "value": " 3.46k/3.46k [00:00&lt;00:00, 242kB/s]"
          }
        },
        "8017e9df46f64b12b68d675e159bad10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "414bb442470a41b994d385ad5d9f2b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c75d847afa7a4571969b597609f2392c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc1698ef3ee1437b88028a1f72328959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a81c8c57dff2427fa9a7a4e466bb9acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "732c175e72f8480d898c9fa2063d7515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9dbb14ed9dd4c23bac9b14fc2972434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f6a73095c9942c7874788c4adb3d7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72679154f1f44abf949f1fc9b17cb528",
              "IPY_MODEL_3fc66a1318c343a48f87c7c384755325",
              "IPY_MODEL_256609dc40c5408a98b21f44096e766e"
            ],
            "layout": "IPY_MODEL_6da635bc3923481996290bcf3eb7b73a"
          }
        },
        "72679154f1f44abf949f1fc9b17cb528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8689d0c9709d48f69a13d2605265b1a6",
            "placeholder": "​",
            "style": "IPY_MODEL_e944858b6b4b411daa09d52a73f69ad9",
            "value": "Downloading readme: 100%"
          }
        },
        "3fc66a1318c343a48f87c7c384755325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba13ff6660ae450c9fba767468c6f075",
            "max": 8205,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_689e29525628402ab2b1885d01ff49d8",
            "value": 8205
          }
        },
        "256609dc40c5408a98b21f44096e766e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c870ecb434454c3bba6f6b86fe34e454",
            "placeholder": "​",
            "style": "IPY_MODEL_64af435277e3441aa12d32cd3b494769",
            "value": " 8.21k/8.21k [00:00&lt;00:00, 507kB/s]"
          }
        },
        "6da635bc3923481996290bcf3eb7b73a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8689d0c9709d48f69a13d2605265b1a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e944858b6b4b411daa09d52a73f69ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba13ff6660ae450c9fba767468c6f075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "689e29525628402ab2b1885d01ff49d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c870ecb434454c3bba6f6b86fe34e454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64af435277e3441aa12d32cd3b494769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d3b246beb4a431e904ee1b91820077c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6427b2983554e2490e4da934642d31a",
              "IPY_MODEL_9e9119f087f94d91b9aaf4adf1e38531",
              "IPY_MODEL_8d423d5417c64da69a1fb39e3920fe8c"
            ],
            "layout": "IPY_MODEL_22682fa4bcc245a2b1c6bb584c942786"
          }
        },
        "b6427b2983554e2490e4da934642d31a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f8bcdb9dee24c97bf4043d1bf64a144",
            "placeholder": "​",
            "style": "IPY_MODEL_6ec0d23cbed3464c812662e75caad813",
            "value": "Generating train split: "
          }
        },
        "9e9119f087f94d91b9aaf4adf1e38531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34fbc80ef9b4416bb67c09825c3d0afb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a16776f0ab894d64aff0080619716cad",
            "value": 1
          }
        },
        "8d423d5417c64da69a1fb39e3920fe8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d32b47f7af4e465ab9c6651cdace416e",
            "placeholder": "​",
            "style": "IPY_MODEL_cd79723c9f324b7aa4d2907cb85f1930",
            "value": " 100/0 [00:00&lt;00:00, 2820.65 examples/s]"
          }
        },
        "22682fa4bcc245a2b1c6bb584c942786": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f8bcdb9dee24c97bf4043d1bf64a144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec0d23cbed3464c812662e75caad813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34fbc80ef9b4416bb67c09825c3d0afb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a16776f0ab894d64aff0080619716cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d32b47f7af4e465ab9c6651cdace416e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd79723c9f324b7aa4d2907cb85f1930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb5de5c968d041bea3e49a87b3844ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4decf20481f4f7abbeb0668e3909056",
              "IPY_MODEL_c80f15a92a934863a94892775abd9ffb",
              "IPY_MODEL_c7689a3f6e0243b3b55348f6ecef7a4f"
            ],
            "layout": "IPY_MODEL_ca098601bf084ac5a8bcc73044c9314f"
          }
        },
        "d4decf20481f4f7abbeb0668e3909056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d833d8b75ea4a18bf6bf2cb915aae0f",
            "placeholder": "​",
            "style": "IPY_MODEL_56c4d453583e4e4a8cac317fcf31b1ce",
            "value": "Generating train split: "
          }
        },
        "c80f15a92a934863a94892775abd9ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c188fed024d4cd2935a1163b6aacab6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e4d9146300345258011adbabb83de30",
            "value": 1
          }
        },
        "c7689a3f6e0243b3b55348f6ecef7a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85035b4bc9ca483bbc96eb518609e993",
            "placeholder": "​",
            "style": "IPY_MODEL_3033f42f25644986b9cee04ab71fa831",
            "value": " 100/0 [00:00&lt;00:00, 2880.47 examples/s]"
          }
        },
        "ca098601bf084ac5a8bcc73044c9314f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d833d8b75ea4a18bf6bf2cb915aae0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c4d453583e4e4a8cac317fcf31b1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c188fed024d4cd2935a1163b6aacab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8e4d9146300345258011adbabb83de30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85035b4bc9ca483bbc96eb518609e993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3033f42f25644986b9cee04ab71fa831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86f623ae1516421b91b965d29fb00efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_478a7245d08f46cc930ac51b26822e7e",
              "IPY_MODEL_95d300ced17a47a3a3ecd997deac357c",
              "IPY_MODEL_e9146f29d09b451dad6044e23f466483"
            ],
            "layout": "IPY_MODEL_df91ae215f24486b998d8c265d514e15"
          }
        },
        "478a7245d08f46cc930ac51b26822e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0781f56c25ae4b05863903b570e42d7c",
            "placeholder": "​",
            "style": "IPY_MODEL_1b4607e0b395407495cf53ea01f62272",
            "value": "Uploading the dataset shards: 100%"
          }
        },
        "95d300ced17a47a3a3ecd997deac357c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_564796abb9e04ebdb264a87d096ce3d1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e996f04104624c07bf230a1a48611da7",
            "value": 1
          }
        },
        "e9146f29d09b451dad6044e23f466483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a808c30182448c6a26bc12f27f4a3a7",
            "placeholder": "​",
            "style": "IPY_MODEL_e9309c5a93594080b43accda26fa8705",
            "value": " 1/1 [00:01&lt;00:00,  1.15s/it]"
          }
        },
        "df91ae215f24486b998d8c265d514e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0781f56c25ae4b05863903b570e42d7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b4607e0b395407495cf53ea01f62272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "564796abb9e04ebdb264a87d096ce3d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e996f04104624c07bf230a1a48611da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a808c30182448c6a26bc12f27f4a3a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9309c5a93594080b43accda26fa8705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abd68fe51c6f4792afdf1981225077e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83b92d152e824a369db5b4d6f13fde31",
              "IPY_MODEL_68417327785b47ed8ee3b14a2c9ee0b1",
              "IPY_MODEL_4cf483b11a764569959b0b4ae829208b"
            ],
            "layout": "IPY_MODEL_b1791cb3b1f440a5b8c418faa8f9b40a"
          }
        },
        "83b92d152e824a369db5b4d6f13fde31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aeb63b5d75d64027887d19f88c73a895",
            "placeholder": "​",
            "style": "IPY_MODEL_ad4073e6e94a469fa264762cabbb0947",
            "value": "Creating parquet from Arrow format: 100%"
          }
        },
        "68417327785b47ed8ee3b14a2c9ee0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8a1d86a1c2944d1baa57b838831b162",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5635167f2c914f99bc350aaaffc59006",
            "value": 1
          }
        },
        "4cf483b11a764569959b0b4ae829208b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e509357275743139979aa1b01de3a96",
            "placeholder": "​",
            "style": "IPY_MODEL_dd05c4b4f5ee4f7dba584f9f38805bfa",
            "value": " 1/1 [00:00&lt;00:00, 51.21ba/s]"
          }
        },
        "b1791cb3b1f440a5b8c418faa8f9b40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeb63b5d75d64027887d19f88c73a895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad4073e6e94a469fa264762cabbb0947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8a1d86a1c2944d1baa57b838831b162": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5635167f2c914f99bc350aaaffc59006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e509357275743139979aa1b01de3a96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd05c4b4f5ee4f7dba584f9f38805bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5ea65af2a4043bdbae637eef7009e0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7268f078743d4a28a2cd309185c97177",
              "IPY_MODEL_628342b29d05408bb67333620c9e2961",
              "IPY_MODEL_f8f970fcecab44b3a324d926a19a1d00"
            ],
            "layout": "IPY_MODEL_841d3392689c4453b2a800ca6df5d80e"
          }
        },
        "7268f078743d4a28a2cd309185c97177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cb054efd5e94777ad128c4523d260c5",
            "placeholder": "​",
            "style": "IPY_MODEL_c1fe7f3a1e394631b76480326334ecfc",
            "value": "Downloading readme: 100%"
          }
        },
        "628342b29d05408bb67333620c9e2961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7ab75e3c3084af1be800bb890480c5d",
            "max": 306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d800aff5e2474522a2ab204755ec5f78",
            "value": 306
          }
        },
        "f8f970fcecab44b3a324d926a19a1d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eced8007dd1480cb8afee1a5c501e57",
            "placeholder": "​",
            "style": "IPY_MODEL_f0c8ff4dc89849ecafa2527f673699ef",
            "value": " 306/306 [00:00&lt;00:00, 15.7kB/s]"
          }
        },
        "841d3392689c4453b2a800ca6df5d80e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cb054efd5e94777ad128c4523d260c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1fe7f3a1e394631b76480326334ecfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7ab75e3c3084af1be800bb890480c5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d800aff5e2474522a2ab204755ec5f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1eced8007dd1480cb8afee1a5c501e57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0c8ff4dc89849ecafa2527f673699ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "399d313b96b9484bb96108de05554e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d432cffe9b9a4dd09ee6581cae0bf3e2",
              "IPY_MODEL_ed9b74818a6d41528112be1927c37518",
              "IPY_MODEL_bf47358c42364d1a969f2c18bd012c2e"
            ],
            "layout": "IPY_MODEL_900ce173d2114f14997a2d0e4d5a364a"
          }
        },
        "d432cffe9b9a4dd09ee6581cae0bf3e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a33a10a23a94a119d635b10d1ecf227",
            "placeholder": "​",
            "style": "IPY_MODEL_7cec1b785d98480aadfc06ee89b28b5f",
            "value": "Downloading data: 100%"
          }
        },
        "ed9b74818a6d41528112be1927c37518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b7cf373e19e45a39eec98615392580e",
            "max": 25425,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce8540f009ef4ba4ba661167df33c2d4",
            "value": 25425
          }
        },
        "bf47358c42364d1a969f2c18bd012c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41f5d7fde4e14584adbeb7e89e4f5747",
            "placeholder": "​",
            "style": "IPY_MODEL_6dfe80ee3e7c4876a24ed536218e710f",
            "value": " 25.4k/25.4k [00:00&lt;00:00, 42.8kB/s]"
          }
        },
        "900ce173d2114f14997a2d0e4d5a364a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a33a10a23a94a119d635b10d1ecf227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cec1b785d98480aadfc06ee89b28b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b7cf373e19e45a39eec98615392580e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce8540f009ef4ba4ba661167df33c2d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41f5d7fde4e14584adbeb7e89e4f5747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dfe80ee3e7c4876a24ed536218e710f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa6d029de1994d1da20e5c0e75550b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57b97368d5d542ba88b30d89169e0f32",
              "IPY_MODEL_d55c29b5d73c4eb6b798fb3002291458",
              "IPY_MODEL_75f19ea1202943b4bb12f9fc280978e6"
            ],
            "layout": "IPY_MODEL_3d76c8fb364f47499dc5bff15657490a"
          }
        },
        "57b97368d5d542ba88b30d89169e0f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9803ca5b3cce435897136da2b68a5ec0",
            "placeholder": "​",
            "style": "IPY_MODEL_ed1ee0a747114b3baf151dcfd4af122f",
            "value": "Generating train split: 100%"
          }
        },
        "d55c29b5d73c4eb6b798fb3002291458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2810bcee9683431d86071c1990e7a797",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f1c0c0ce4714d2b93e17a2e029a8ba0",
            "value": 100
          }
        },
        "75f19ea1202943b4bb12f9fc280978e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52bf70270b0f4bab8c0541082ee60553",
            "placeholder": "​",
            "style": "IPY_MODEL_f35da28af5ac45138d5cc945df6db0f5",
            "value": " 100/100 [00:00&lt;00:00, 3080.40 examples/s]"
          }
        },
        "3d76c8fb364f47499dc5bff15657490a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9803ca5b3cce435897136da2b68a5ec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed1ee0a747114b3baf151dcfd4af122f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2810bcee9683431d86071c1990e7a797": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f1c0c0ce4714d2b93e17a2e029a8ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52bf70270b0f4bab8c0541082ee60553": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f35da28af5ac45138d5cc945df6db0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJzEzluvIb5v",
        "outputId": "78a05ac1-a363-43c0-ff49-f5462e936490"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLt1i_ayIg8H",
        "outputId": "b85988be-7889-4fee-8b55-675ca524eb3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/547.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m542.7/547.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xi4WnMCpIO4I"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pretrained_dataset = load_dataset(\"EleutherAI/pile\", split=\"train\", streaming=True)\n",
        "\n",
        "pretrained_dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "a53346ec85c64af3a3e57de160873b25",
            "f373c50c5e0e477fb05352fe1b84db3c",
            "a17d49adfbf84c44baf37669fa4728c8",
            "84cd6ea8c8724f678b1b5a333f32a761",
            "8017e9df46f64b12b68d675e159bad10",
            "414bb442470a41b994d385ad5d9f2b7b",
            "c75d847afa7a4571969b597609f2392c",
            "cc1698ef3ee1437b88028a1f72328959",
            "a81c8c57dff2427fa9a7a4e466bb9acf",
            "732c175e72f8480d898c9fa2063d7515",
            "e9dbb14ed9dd4c23bac9b14fc2972434",
            "0f6a73095c9942c7874788c4adb3d7d3",
            "72679154f1f44abf949f1fc9b17cb528",
            "3fc66a1318c343a48f87c7c384755325",
            "256609dc40c5408a98b21f44096e766e",
            "6da635bc3923481996290bcf3eb7b73a",
            "8689d0c9709d48f69a13d2605265b1a6",
            "e944858b6b4b411daa09d52a73f69ad9",
            "ba13ff6660ae450c9fba767468c6f075",
            "689e29525628402ab2b1885d01ff49d8",
            "c870ecb434454c3bba6f6b86fe34e454",
            "64af435277e3441aa12d32cd3b494769"
          ]
        },
        "id": "FV1e-3UUIZ26",
        "outputId": "9379991d-2af3-4406-a4af-32c5fd1f0bc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/3.46k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a53346ec85c64af3a3e57de160873b25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f6a73095c9942c7874788c4adb3d7d3"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for c4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/c4.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/huggingface/modules/datasets_modules/datasets/c4/584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db/c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm7kJo4ZI0Ec",
        "outputId": "cbecf47a-6dca-406e-f3df-df04e7c920db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IterableDataset({\n",
              "    features: ['text', 'timestamp', 'url'],\n",
              "    n_shards: 1024\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "print(\"Pretrained dataset:\")\n",
        "top_n = itertools.islice(pretrained_dataset, n)\n",
        "for i in top_n:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRC1jbCFI7Jy",
        "outputId": "1b5731b7-9682-492e-d0dd-dd7804ec5d8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained dataset:\n",
            "{'text': 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.', 'timestamp': '2019-04-25T12:57:54Z', 'url': 'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/'}\n",
            "{'text': 'Discussion in \\'Mac OS X Lion (10.7)\\' started by axboi87, Jan 20, 2012.\\nI\\'ve got a 500gb internal drive and a 240gb SSD.\\nWhen trying to restore using disk utility i\\'m given the error \"Not enough space on disk ____ to restore\"\\nBut I shouldn\\'t have to do that!!!\\nAny ideas or workarounds before resorting to the above?\\nUse Carbon Copy Cloner to copy one drive to the other. I\\'ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\\'t be bootable. CCC usually works in \"file mode\" and it can easily copy a larger drive (that\\'s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\\nI\\'ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\\'t fit is there was slightly more than 4 GB of data.', 'timestamp': '2019-04-21T10:07:13Z', 'url': 'https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/'}\n",
            "{'text': 'Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.', 'timestamp': '2019-04-25T10:40:23Z', 'url': 'https://awishcometrue.com/Catalogs/Clearance/Tweens/V1960-Find-A-Way'}\n",
            "{'text': \"How many backlinks per day for new site?\\nDiscussion in 'Black Hat SEO' started by Omoplata, Dec 3, 2010.\\n1) for a newly created site, what's the max # backlinks per day I should do to be safe?\\n2) how long do I have to let my site age before I can start making more blinks?\\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?\", 'timestamp': '2019-04-21T12:46:19Z', 'url': 'https://www.blackhatworld.com/seo/how-many-backlinks-per-day-for-new-site.258615/'}\n",
            "{'text': 'The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what’s included in the mill levy measure.', 'timestamp': '2019-04-20T14:33:21Z', 'url': 'http://bond.dpsk12.org/category/news/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=1\n",
        "print(\"Pretrained Datasets:\")\n",
        "top_n=itertools.islice(pretrained_dataset,n)\n",
        "for i in top_n:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it2aWCz4JDYs",
        "outputId": "a286c1b2-8d89-4d2b-f4e8-42662a603419"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained Datasets:\n",
            "{'text': 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.', 'timestamp': '2019-04-25T12:57:54Z', 'url': 'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename=\"/content/mydata - Sheet1.csv\"\n",
        "instruction_dataset_df=pd.read_csv(filename)\n",
        "\n",
        "instruction_dataset_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PnXvFUoYK9g-",
        "outputId": "3f4dca9c-fd5e-42a6-997d-7982068d8397"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             question  \\\n",
              "0   How can I evaluate the performance and quality...   \n",
              "1   Can I find information about the code's approa...   \n",
              "2   How does Lamini AI handle requests for generat...   \n",
              "3   Does the `submit_job()` function expose any ad...   \n",
              "4   Does the `add_data()` function support differe...   \n",
              "..                                                ...   \n",
              "95  How can I handle bias or sensitive content in ...   \n",
              "96  Do I need to pay money to use Lamini's functio...   \n",
              "97  Does Lamini support generating code for natura...   \n",
              "98  Does the documentation provide information abo...   \n",
              "99  How does Lamini differ from ChatGPT? What are ...   \n",
              "\n",
              "                                               answer  \n",
              "0   There are several metrics that can be used to ...  \n",
              "1   Yes, the code includes methods for submitting ...  \n",
              "2   Lamini AI offers features for generating text ...  \n",
              "3   It is unclear which `submit_job()` function is...  \n",
              "4   No, the `add_data()` function does not support...  \n",
              "..                                                ...  \n",
              "95  To handle bias or sensitive content in the gen...  \n",
              "96  Lamini presents a nuanced pricing structure th...  \n",
              "97  Yes, Lamini can generate code for natural lang...  \n",
              "98  If you care about data privacy and storage, La...  \n",
              "99  Lamini and ChatGPT differ in their core functi...  \n",
              "\n",
              "[100 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1116d537-3302-4729-a6ce-a0ca40cecd4e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How can I evaluate the performance and quality...</td>\n",
              "      <td>There are several metrics that can be used to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Can I find information about the code's approa...</td>\n",
              "      <td>Yes, the code includes methods for submitting ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does Lamini AI handle requests for generat...</td>\n",
              "      <td>Lamini AI offers features for generating text ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Does the `submit_job()` function expose any ad...</td>\n",
              "      <td>It is unclear which `submit_job()` function is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Does the `add_data()` function support differe...</td>\n",
              "      <td>No, the `add_data()` function does not support...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>How can I handle bias or sensitive content in ...</td>\n",
              "      <td>To handle bias or sensitive content in the gen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Do I need to pay money to use Lamini's functio...</td>\n",
              "      <td>Lamini presents a nuanced pricing structure th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Does Lamini support generating code for natura...</td>\n",
              "      <td>Yes, Lamini can generate code for natural lang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Does the documentation provide information abo...</td>\n",
              "      <td>If you care about data privacy and storage, La...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>How does Lamini differ from ChatGPT? What are ...</td>\n",
              "      <td>Lamini and ChatGPT differ in their core functi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1116d537-3302-4729-a6ce-a0ca40cecd4e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1116d537-3302-4729-a6ce-a0ca40cecd4e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1116d537-3302-4729-a6ce-a0ca40cecd4e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6ce11df9-6fa0-40e2-bb22-9ebe826bc7f8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6ce11df9-6fa0-40e2-bb22-9ebe826bc7f8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6ce11df9-6fa0-40e2-bb22-9ebe826bc7f8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9d47b81e-f477-43c6-aaff-ebe01c4ebf25\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('instruction_dataset_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9d47b81e-f477-43c6-aaff-ebe01c4ebf25 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('instruction_dataset_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "instruction_dataset_df",
              "summary": "{\n  \"name\": \"instruction_dataset_df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 99,\n        \"samples\": [\n          \"Is Lamini aquired by Google\",\n          \"Are there any tutorials on using Lamini for generating chatbot responses?\",\n          \"Do I need to pay money to use Lamini's functions, or is it free for kids like me?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 95,\n        \"samples\": [\n          \"The article mentions that Lamini allows for the training of high-performing LLMs on large datasets with just a few lines of code from the Lamini library. Additionally, they have released an open-source instruction-following LLM using Lamini to train the Pythia base model with 37k generated instructions, filtered from 70k. While there is no specific mention of plans to release additional pre-trained LLM models for specific domains or use cases, Lamini is focused on making it easy for engineering teams to train their own LLMs using their own data.\",\n          \"Yes, Lamini requires an internet connection to function as it is a cloud-based AI language model.\",\n          \"Yes, Lamini can be used to create AI-generated content for creative writing, including generating poems and short stories. Lamini\\u2019s python library demonstrates an example of using Lamini to generate a story based on input descriptors such as likes and tone. However, the quality and creativity of the generated content will depend on the specific implementation and training of the Lamini model.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_dataset_df[\"question\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QkhjtpmHLtXW",
        "outputId": "dddfe8d3-ad97-49eb-f733-454365eea4b5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Can I find information about the code's approach to handling long-running tasks and background jobs?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_dataset_df[\"answer\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "81Dm33vmL1rb",
        "outputId": "be818d5f-8498-4b92-eb47-e5ae0bed3dd3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, the code includes methods for submitting jobs, checking job status, and retrieving job results. It also includes a method for canceling jobs. Additionally, there is a method for sampling multiple outputs from a model, which could be useful for long-running tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = instruction_dataset_df.to_dict()\n",
        "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "FaQNC6T8JfVn",
        "outputId": "88823117-1a68-44c4-b693-533311fcb776"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"How can I evaluate the performance and quality of the generated text from Lamini models?There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example=instruction_dataset_df.to_dict()\n",
        "text=examples[\"question\"][1]+examples[\"answer\"][1]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "VnbrrE_0JoqT",
        "outputId": "0cc4d8b4-0187-4403-8a93-acdb91d95df7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Can I find information about the code's approach to handling long-running tasks and background jobs?Yes, the code includes methods for submitting jobs, checking job status, and retrieving job results. It also includes a method for canceling jobs. Additionally, there is a method for sampling multiple outputs from a model, which could be useful for long-running tasks.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"question\" in examples and \"answer\" in examples:\n",
        "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "elif \"instruction\" in examples and \"response\" in examples:\n",
        "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
        "elif \"input\" in examples and \"output\" in examples:\n",
        "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "else:\n",
        "  text = examples[\"text\"][0]"
      ],
      "metadata": {
        "id": "B0bsUFrsLms4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_qa = \"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\n",
        "{answer}\"\"\""
      ],
      "metadata": {
        "id": "8ZB-qKw-MIe0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_template_qa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXrQUNgLMkmz",
        "outputId": "87606f4f-a19d-4d28-caf9-76392ad9b0f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Question:\n",
            "{question}\n",
            "\n",
            "### Answer:\n",
            "{answer}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question=examples[\"question\"][0]\n",
        "print(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ghIIgV8M6E9",
        "outputId": "343fce86-3bcb-438e-a8dd-0982da824a3a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can I evaluate the performance and quality of the generated text from Lamini models?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = examples[\"answer\"][0]\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I79DZBRzNCeO",
        "outputId": "98629ae1-3805-430a-9d51-a9e0c36db1cc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_with_prompt_template=prompt_template_qa.format(question=question,answer=answer)\n",
        "print(text_with_prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYV3r1gXNI5G",
        "outputId": "ae50dbf9-7c0b-4e61-b193-9dd9de00371b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Question:\n",
            "How can I evaluate the performance and quality of the generated text from Lamini models?\n",
            "\n",
            "### Answer:\n",
            "There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_q=\"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\"\"\"\n"
      ],
      "metadata": {
        "id": "VGn0vaEDM3nG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is for entire data set"
      ],
      "metadata": {
        "id": "4wiHf4dPN3Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = len(examples[\"question\"])"
      ],
      "metadata": {
        "id": "z2g2unXANyyL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C94RSAn8OAjD",
        "outputId": "8dc1d699-be12-4b38-d960-0120897738df"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_ans = len(examples[\"answer\"])"
      ],
      "metadata": {
        "id": "fvMSo1F4OHtg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlpRrVd7OK2H",
        "outputId": "d103b2b7-2330-4722-e208-bf215372bc0e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_text_only = []\n",
        "finetuning_dataset_question_answer = []\n",
        "for i in range(num_examples):\n",
        "  question = examples[\"question\"][i]\n",
        "  answer = examples[\"answer\"][i]\n",
        "\n",
        "  text_with_prompt_template_qa = prompt_template_qa.format(question=question, answer=answer)\n",
        "  finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
        "\n",
        "  text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
        "  finetuning_dataset_question_answer.append({\"question\": text_with_prompt_template_q, \"answer\": answer})"
      ],
      "metadata": {
        "id": "SgVVYZ2XODQK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_text_only"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqVISsUiO0uk",
        "outputId": "16d9c016-0e1b-4240-d25a-1457b7c97b26"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': \"### Question:\\nHow can I evaluate the performance and quality of the generated text from Lamini models?\\n\\n### Answer:\\nThere are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\"},\n",
              " {'text': \"### Question:\\nCan I find information about the code's approach to handling long-running tasks and background jobs?\\n\\n### Answer:\\nYes, the code includes methods for submitting jobs, checking job status, and retrieving job results. It also includes a method for canceling jobs. Additionally, there is a method for sampling multiple outputs from a model, which could be useful for long-running tasks.\"},\n",
              " {'text': '### Question:\\nHow does Lamini AI handle requests for generating text that requires reasoning or decision-making based on given information?\\n\\n### Answer:\\nLamini AI offers features for generating text that requires logical reasoning or inference beyond simple text generation. It can handle user prompts that involve complex reasoning or logical inference, and can generate text that captures the nuances of different cultural or regional variations.'},\n",
              " {'text': '### Question:\\nDoes the `submit_job()` function expose any advanced training options such as learning rate schedules or early stopping?\\n\\n### Answer:\\nIt is unclear which `submit_job()` function is being referred to as there is no such function defined in Lamini’s python library snippets. Please provide more information or context to answer the question accurately.'},\n",
              " {'text': \"### Question:\\nDoes the `add_data()` function support different data augmentation techniques or preprocessing options for training data?\\n\\n### Answer:\\nNo, the `add_data()` function does not support different data augmentation techniques or preprocessing options for training data. It simply adds the provided examples to the program's list of examples.\"},\n",
              " {'text': '### Question:\\nCan Lamini generate text for data storytelling or data visualization purposes?\\n\\n### Answer:\\nYes, Lamini can generate text for data storytelling or data visualization purposes using its natural language generation capabilities.'},\n",
              " {'text': '### Question:\\nCan the documentation predict the outcome of a coin toss?\\n\\n### Answer:\\nI wish! This documentation only talks about Lamini’s LLM Engine.'},\n",
              " {'text': '### Question:\\nHow does the `submit_job()` function work in Lamini? What does it mean to submit a job, and what happens behind the scenes?\\n\\n### Answer:\\nThe `submit_job()` function in Lamini is used to submit a job for training a machine learning model. When you submit a job, Lamini takes the training data and uses it to train a model based on the specified parameters. Behind the scenes, Lamini uses distributed computing to train the model on multiple machines, which allows for faster training times. Once the training is complete, the resulting model is saved and can be used for inference.'},\n",
              " {'text': '### Question:\\nDoes Lamini support generating code\\n\\n### Answer:\\nYes, Lamini supports generating code through its API.'},\n",
              " {'text': '### Question:\\nCan Lamini be used to create chatbots or virtual assistants?\\n\\n### Answer:\\nYes, Lamini can be used to build conversational AI agents or chatbots. It provides tools and functionalities for generating coherent and contextually appropriate responses in conversational settings, as well as support for multi-turn conversations and context-aware recommendation systems.'},\n",
              " {'text': '### Question:\\nHow can Lamini be used to generate text with specific stylistic attributes, such as poetic language or persuasive rhetoric?\\n\\n### Answer:\\nLamini can be trained to generate text with specific stylistic attributes by fine-tuning its language model on a dataset that includes examples of the desired style. For example, to generate text with poetic language, the model can be trained on a corpus of poetry. Similarly, to generate text with persuasive rhetoric, the model can be trained on a dataset of persuasive speeches or advertisements. By adjusting the training data and fine-tuning the model, Lamini can be customized to generate text with a wide range of stylistic attributes.'},\n",
              " {'text': '### Question:\\nIs it possible to fine-tune Lamini on a small dataset with limited annotations?\\n\\n### Answer:\\nYes, it is possible to fine-tune Lamini on a small dataset with limited annotations using the DatasetBalancer class in the balancer.py file. The stochastic_balance_dataset and full_balance_dataset methods can be used to balance the dataset with embeddings and improve the performance of the model.'},\n",
              " {'text': '### Question:\\nHow can I handle long texts or documents when using Lamini? Are there any limitations or considerations?\\n\\n### Answer:\\nLamini can handle long or complex documents during the training process, but there may be limitations or considerations depending on the available computational resources and the specific task or model architecture. It is recommended to preprocess the input data and split it into smaller chunks or batches to improve efficiency and avoid memory issues. Additionally, it may be necessary to adjust the hyperparameters or use specialized techniques such as hierarchical or attention-based models to handle long sequences effectively. The Lamini documentation provides guidelines and best practices for handling long texts or documents, and it is recommended to consult it for more information.'},\n",
              " {'text': '### Question:\\nHow do I report a bug or issue with the Lamini documentation?\\n\\n### Answer:\\nYou can report a bug or issue with the Lamini documentation by submitting an issue on the Lamini GitHub page.'},\n",
              " {'text': '### Question:\\nCan Lamini be used in an online learning setting, where the model is updated continuously as new data becomes available?\\n\\n### Answer:\\nIt is possible to use Lamini in an online learning setting where the model is updated continuously as new data becomes available. However, this would require some additional implementation and configuration to ensure that the model is updated appropriately and efficiently.'},\n",
              " {'text': \"### Question:\\nWhat is the company culture that Lamini AI values?\\n\\n### Answer:\\nLamini AI believes in the following:\\\\n1. Innovation and Creativity: Lamini AI values a culture of innovation and encourages employees to think creatively, explore new ideas, and push the boundaries of AI technology. This includes fostering an environment that supports experimentation, welcomes novel approaches, and rewards innovative solutions.\\\\n2. Collaboration and Teamwork: Collaboration is essential in AI development. Lamini AI values a culture that promotes teamwork, open communication, and knowledge sharing. Employees are encouraged to collaborate across teams, departments, and disciplines to leverage collective expertise and achieve common goals.\\\\n3. Continuous Learning and Growth: Given the dynamic nature of AI, Lamini AI promotes a culture of continuous learning and growth. Employees are encouraged to expand their knowledge, stay updated with the latest AI advancements, and pursue professional development opportunities. This may include providing resources for training, attending conferences, and fostering a culture of intellectual curiosity.\\\\n4. Diversity and Inclusion: Building a diverse and inclusive workforce is a priority for Lamini AI, which values diversity in its team, recognizing the importance of different perspectives, backgrounds, and experiences in driving innovation and creativity. The company fostes an inclusive culture that promotes equality, respect, and opportunities for all employees.\\\\n5. Customer-Centric Approach: Lamini AI is focused on providing value to its customers. The company has a customer-centric culture that emphasizes understanding customer needs, delivering exceptional user experiences, and providing prompt and effective support. Customer feedback and satisfaction plays a vital role in shaping the company's products and services.\"},\n",
              " {'text': \"### Question:\\nCan Lamini generate text that simulates a particular persona or writing style, such as mimicking famous authors or historical figures?\\n\\n### Answer:\\nYes, Lamini has the capability to generate text that emulates the style of famous authors or literary figures, as well as mimicking the writing style of a specific time period, such as the Victorian era or the Renaissance. This can be achieved through fine-tuning Lamini's language models with specific training data and prompts that reflect the desired persona or writing style. However, it is important to note that the quality and accuracy of the generated text may vary depending on the complexity and specificity of the desired persona or style.\"},\n",
              " {'text': '### Question:\\nWhat is Lamini and what is its collaboration with Databricks about?\\n\\n### Answer:\\nLamini is a library that simplifies the process of accessing data in Databricks. The collaboration between Lamini and Databricks involves integrating Lamini with the Databricks platform to provide a seamless experience for users.'},\n",
              " {'text': \"### Question:\\nAre you prompt-engineered?\\n\\n### Answer:\\nSorry I can't reveal that information to you. I am trained by the amazing team at Lamini, to know more contact them\"},\n",
              " {'text': '### Question:\\nnan\\n\\n### Answer:\\nDid you type something, I am having trouble understanding'},\n",
              " {'text': '### Question:\\nCan Lamini generate code snippets or programming examples for different programming languages?\\n\\n### Answer:\\nYes, Lamini can generate code snippets and provide programming assistance for specific languages during the customization process of a language model.'},\n",
              " {'text': '### Question:\\nAre there any tutorials on using Lamini for content generation in storytelling games?\\n\\n### Answer:\\nLamini has many tutorials and examples of how to use its LLM Engine available in its documentation, which you can find online through lamini.ai. You can easily adapt those instructions to any application involving an LLM that you see fit!'},\n",
              " {'text': '### Question:\\nDoes Lamini require an internet connection to function?\\n\\n### Answer:\\nYes, Lamini requires an internet connection to function as it is a cloud-based AI language model.'},\n",
              " {'text': '### Question:\\nCan you explain the CC-BY license mentioned? What does it allow me to do with the customized language model?\\n\\n### Answer:\\nLamini allows for commercial use of their LLM technology under a permissive Apache 2.0 license unless otherwise specified. You keep access and ownership of your own data, and we don’t use your data to train models for anyone else but you. For more information, please reach out to Lamini directly.'},\n",
              " {'text': '### Question:\\nWhat are the benefits of using the Lamini data generator for LLM training?\\n\\n### Answer:\\nThe benefits of using the Lamini data generator for LLM training include the ability to generate high-quality, diverse datasets that can improve the performance and accuracy of language models. The data generator can also be customized for specific use cases or vertical-specific languages, and can handle data preprocessing tasks such as tokenization and data cleaning. Additionally, the generated dataset is available for commercial use, and the data generator pipeline can be optimized to reduce performance plateaus and improve training efficiency.'},\n",
              " {'text': '### Question:\\nAre there any success stories or case studies showcasing how Lamini has been used by other enterprise organizations?\\n\\n### Answer:\\nYes, there are several success stories and case studies showcasing how Lamini has been used by other enterprise organizations. For example, Lamini has been used by companies in the financial industry to generate financial reports and by healthcare organizations to generate medical reports. Lamini has also been used by e-commerce companies to generate product descriptions and by social media companies to generate captions for images. These success stories demonstrate the versatility and effectiveness of Lamini in various industries and use cases.'},\n",
              " {'text': '### Question:\\nCan Lamini be used for multiple languages, or is it primarily focused on English?\\n\\n### Answer:\\nLLM Engine Lamini can be used for multiple languages, not just English.'},\n",
              " {'text': \"### Question:\\nAre there any known challenges or trade-offs associated with using Lamini for model customization tasks?\\n\\n### Answer:\\nYes, there are certain challenges and trade-offs associated with using Lamini for model customization tasks. Some of them include:\\\\nLimited control over the base model: While Lamini allows customization of language models, the level of control over the base model's architecture and inner workings may be limited. This can restrict the extent of customization possible.\\\\nFine-tuning data requirements: To achieve optimal results, fine-tuning typically requires a significant amount of high-quality data. Acquiring and curating such data can be time-consuming and resource-intensive.\\\\nGeneralization to specific use cases: Fine-tuning a language model on a specific dataset may result in overfitting, where the model performs well on the training data but struggles with generalizing to unseen examples. Balancing model performance and generalization is an ongoing challenge.\\\\nBias and fairness considerations: Language models trained on existing datasets can inherit biases present in the data. When fine-tuning or customizing models, it's important to be mindful of potential biases and take steps to mitigate them.\\\\nComputational resources and time: Training and fine-tuning language models can require significant computational resources, such as GPUs or TPUs, and can be time-consuming. This can limit the accessibility and practicality of customization for certain individuals or organizations.\\\\nEthical considerations: As with any powerful AI technology, there are ethical considerations surrounding its use. Customizing language models should be done responsibly, considering issues like privacy, security, and potential misuse.\"},\n",
              " {'text': '### Question:\\nCan I use Lamini as api instead of python library\\n\\n### Answer:\\nYes, we have a REST API available. To see documentation go to https://lamini-ai.github.io/'},\n",
              " {'text': \"### Question:\\nAre there any alternatives to Lamini that offer similar functionality?\\n\\n### Answer:\\nYes, there are several alternatives to Lamini that offer similar functionality. Some popular options include OpenAI's GPT-3, Google's BERT, and Hugging Face's Transformers. Each of these models has its own strengths and weaknesses, so it's important to evaluate them based on your specific needs and use case.\"},\n",
              " {'text': '### Question:\\nDoes Lamini support generating code for information extraction tasks?\\n\\n### Answer:\\nYes, Lamini can generate code for information extraction tasks.'},\n",
              " {'text': '### Question:\\nDoes Lamini support incremental learning or continuous improvement of LLMs based on real-time data streams?\\n\\n### Answer:\\nThe article does not mention whether Lamini supports incremental learning or continuous improvement of LLMs based on real-time data streams.'},\n",
              " {'text': '### Question:\\nCan Lamini generate text for generating personalized emails or newsletters?\\n\\n### Answer:\\nYes, Lamini can generate text for generating personalized emails or newsletters. However, the specific capabilities and features may vary depending on the specific Lamini tool or platform being used.'},\n",
              " {'text': '### Question:\\nIs Lamini a free tool to use, or are there any costs associated with its usage?\\n\\n### Answer:\\nLamini provides every user with 10000 free credits. After that, you can purchase more credits in the “API” tab at app.lamini.ai.'},\n",
              " {'text': '### Question:\\nAre there any rate limits or restrictions on the usage of the `submit_job()` function in Lamini?\\n\\n### Answer:\\nYes, there are rate limits on the usage of the `submit_job()` function in Lamini. If you encounter a rate limit error, the `RateLimitError` exception will be raised.'},\n",
              " {'text': \"### Question:\\nCan Lamini handle large-scale distributed training across multiple machines or clusters?\\n\\n### Answer:\\nYes, Lamini can handle large-scale distributed training across multiple machines or clusters. It uses a distributed training framework based on PyTorch's DistributedDataParallel module, which allows for efficient parallelization of training across multiple GPUs or machines. This enables faster training times and the ability to handle larger datasets.\"},\n",
              " {'text': '### Question:\\nDoes Lamini support generating code for machine learning models?\\n\\n### Answer:\\nYes, Lamini supports generating code for machine learning models through its Llama large language model engine.'},\n",
              " {'text': '### Question:\\nOn which data were you trained?\\n\\n### Answer:\\nI am trained by amazing team at Lamini, to know more contact them'},\n",
              " {'text': '### Question:\\nHow do I uninstall Lamini from my system?\\n\\n### Answer:\\nTo uninstall Lamini from your system, open a command prompt and type: pip uninstall lamini.'},\n",
              " {'text': '### Question:\\nIs it possible to fine-tune Lamini on a specific dataset for dialogue generation tasks?\\n\\n### Answer:\\nYes, it is possible to fine-tune Lamini on a specific dataset for dialogue generation tasks. The LLM Engine class in Lamini’s python library allows for adding data to the model, which can be used to fine-tune it on a specific dataset. Additionally, the add_model method can be used to create multiple models with different parameters and output types.'},\n",
              " {'text': '### Question:\\nHow does Lamini aim to make generative AI more accessible and usable for engineering teams?\\n\\n### Answer:\\nLamini aims to make generative AI more accessible and usable for engineering teams by providing a user-friendly interface and pre-built models that can be fine-tuned for specific use cases. Additionally, Lamini offers features such as prompt-tuning and RLHF to optimize LLM training and reduce performance plateaus. The library also provides tools for data generation, preprocessing, and analysis, as well as support for multiple programming languages and frameworks. Finally, Lamini is designed to be scalable and can handle large datasets efficiently, making it a powerful tool for enterprise-level applications.'},\n",
              " {'text': '### Question:\\nAre there any tutorials on using Lamini for generating chatbot responses?\\n\\n### Answer:\\nYes, there are tutorials available on using Lamini for generating chatbot responses. You can check out the official documentation and examples provided by the Lamini team to get started. Additionally, there are also various online resources and tutorials available that can help you learn how to use Lamini effectively for chatbot development.'},\n",
              " {'text': '### Question:\\nCan you fine-tune an openai model?\\n\\n### Answer:\\nYes! Lamini LLM Engine has fine-tuning support, including base models from hugging face as well as OpenAI. Contact us for access. You can also look at the documentation for llm.add_data, which makes your data available to the LLM Engine. The LLM Engine performs fast training using this data, which should complete in just a few seconds, even for large datasets. Full fine tuning is more expensive, which is why we ask you to contact us to allocate enough compute resources to support it.'},\n",
              " {'text': '### Question:\\nDoes Lamini AI offer any pre-trained models for specific use cases?\\n\\n### Answer:\\nYes, Lamini AI offers pre-trained models for specific use cases.'},\n",
              " {'text': \"### Question:\\nCan Lamini's documentation be used as a secret code to communicate with aliens?\\n\\n### Answer:\\nI wish! This documentation only talks about Lamini’s LLM Engine.\"},\n",
              " {'text': '### Question:\\nAre there any examples or sample code provided in the documentation?\\n\\n### Answer:\\nExamples and sample documentation is available at https://lamini-ai.github.io/. In particular, there is a QA example where we show you how to feed your documentation into a model to ask questions about a code base. Additionally, sample code and colab notebooks are provided and linked throughout the documentation where relevant. Feedback on our documentation is greatly appreciated - we care about making LLMs - and by extension Lamini - easier to use. Please direct any feedback to support@lamini.ai.'},\n",
              " {'text': '### Question:\\nHow does Lamini handle generating text that maintains coherence and logical flow between sentences and paragraphs?\\n\\n### Answer:\\nLamini uses advanced natural language processing techniques to ensure that generated text maintains coherence and logical flow between sentences and paragraphs. This includes analyzing the context and meaning of each sentence and using that information to guide the generation of subsequent sentences. Additionally, Lamini can be fine-tuned and customized for specific tasks or domains to further improve coherence and flow.'},\n",
              " {'text': '### Question:\\nAre there any examples of using Lamini for content generation in marketing campaigns?\\n\\n### Answer:\\nIf you think a large language model can be used for content generation in marketing campaigns, then we think Lamini can help. Recent advances in LLMs have shown that they can write coherent marketing copy. If you have great example data, Lamini can help you finetune a model to suit your writing needs.'},\n",
              " {'text': '### Question:\\nHow do I create a Type class for data in Lamini?\\n\\n### Answer:\\nYou can use the Type and Context classes in the Lamini Python library to create a Type class for data. For example, you can create an Animal type as follows: from llama import Type, Context class Animal(Type): name = str(Context=\"name of the animal\") n_legs = int(Context=\"number of legs that animal has\") llama_animal = Animal(name=\"Larry\", n_legs=4)'},\n",
              " {'text': '### Question:\\nCan Lamini generate code for recommendation systems?\\n\\n### Answer:\\nYes, Lamini can generate code for recommendation systems. Lamini’s python library includes functions for ingesting and generating text, and can generate code if asked.'},\n",
              " {'text': '### Question:\\nWhen using the `get_job_result()` function in Lamini, what kind of output can we expect? How is it structured?\\n\\n### Answer:\\nWhen using the `get_job_result()` function in Lamini, the output we can expect is a JSON object containing information about the job status and the result of the job. The structure of the output includes a \"status\" field indicating whether the job is still running or has completed, a \"result\" field containing the result of the job if it has completed, and an optional \"error\" field containing any error messages if the job has failed.'},\n",
              " {'text': \"### Question:\\nWhat is Lamini AI's stance on diversity and inclusion?\\n\\n### Answer:\\nLamini AI's statement reflects a strong commitment to diversity and inclusion. The company values and promotes a diverse and inclusive work environment where individuals from all backgrounds and identities are respected and provided with equal opportunities. Lamini AI believes that diversity and inclusion are crucial to its success as a company, recognizing the power of diverse perspectives, experiences, and ideas in driving innovation and problem-solving.\"},\n",
              " {'text': '### Question:\\nCan I deploy the customized LLM created with Lamini on various platforms or frameworks? Are there any specific deployment considerations or requirements?\\n\\n### Answer:\\nYes, models can be deployed in any containerized environment. Lamini can also host your models for you. The only requirements are the ability to run docker containers, and to supply powerful enough GPUs to run an LLM.'},\n",
              " {'text': '### Question:\\nCan you explain how the `add_data()` function works in Lamini? Is it like adding more knowledge for the machine?\\n\\n### Answer:\\nYes, the `add_data()` function in Lamini is used to add more examples or data to the program. This helps the machine to learn and improve its performance by having more information to work with. The function can take in a single example or a list of examples, and it appends them to the existing examples in the program. The examples can be of any data type, and the function automatically converts them to a dictionary format using the `value_to_dict()` function.'},\n",
              " {'text': '### Question:\\nDoes Lamini AI provide any features for generating text that incorporates user-provided examples or templates?\\n\\n### Answer:\\nNo, Lamini AI does not provide any features for generating text that incorporates user-provided examples or templates.'},\n",
              " {'text': '### Question:\\nHow does Lamini handle the challenge of overfitting or underfitting during LLM training?\\n\\n### Answer:\\nLamini provides several mechanisms to address the challenge of overfitting or underfitting during LLM training. One approach is to use regularization techniques such as dropout or weight decay to prevent the model from memorizing the training data too closely. Another approach is to use early stopping, where the training is stopped when the validation loss starts to increase, indicating that the model is starting to overfit. Additionally, Lamini supports hyperparameter tuning to find the optimal settings for the model architecture and training parameters.'},\n",
              " {'text': \"### Question:\\nHow does Lamini compare to other existing tools or frameworks for customizing language models? What are its unique features or advantages?\\n\\n### Answer:\\nLamini makes model training, hosting, and deployment easy. Public LLMs, such as ChatGPT, can only take in <1% of your data—whether that be customer support, business intelligence, or clickstream data. To make matters worse, you can’t just hand your most valuable data over, because it’s private. Lamini’s LLM Engine can run in your VPC, securely handling your model's valuable data resources.\"},\n",
              " {'text': '### Question:\\nDoes the documentation provide examples of how to integrate the code with other systems or APIs?\\n\\n### Answer:\\nYes, the documentation provides examples of how to integrate the code with other systems or APIs, more information in setting it up visit: https://lamini-ai.github.io/'},\n",
              " {'text': '### Question:\\nAre there any known limitations or challenges when using Lamini with noisy or unstructured data?\\n\\n### Answer:\\nYes, there are known limitations and challenges when using Lamini with noisy or unstructured data. Since Lamini is designed to work with structured data, it may struggle with unstructured data such as free-form text or data with inconsistent formatting. Additionally, noisy data with errors or inconsistencies may negatively impact the accuracy of the generated text. It is important to preprocess and clean the data before using Lamini to ensure the best results.'},\n",
              " {'text': '### Question:\\nHow does Lamini AI handle ambiguous pronouns or references in the generated text?\\n\\n### Answer:\\nLamini AI does not have any built-in mechanisms to handle ambiguous pronouns or references in the generated text.'},\n",
              " {'text': '### Question:\\nHow can I take cleaned data from databricks and then add it to lamini to fine tune a LLM?\\n\\n### Answer:\\nFirst extract the data from databricks into a pandas dataframe. Then create lamini types with fields corresponding to the columns in the dataframe. Finally add the data to lamini with llm.add_data. Now you have an LLM that you can query, e.g. with llm(...).'},\n",
              " {'text': '### Question:\\nQrdsrqfduwnfkkhopktylhmdknjhqwgsagvsxhajbfnkdmg,mf\\n\\n### Answer:\\nI’m having trouble understanding you. Can you try again?'},\n",
              " {'text': '### Question:\\nWhere can I download the latest version of Python?\\n\\n### Answer:\\nYou can download the latest version of Python from the Python website and run the installer. Alternatively, you can update Python using a package manager such as Homebrew (for macOS) or apt-get (for Linux).'},\n",
              " {'text': '### Question:\\nIs Lamini aquired by Google\\n\\n### Answer:\\nNo, Lamini AI is an independent company working towards providing the users with an interface to create, improve and deploy their own language models using their own data.'},\n",
              " {'text': '### Question:\\nHow does the `sample()` function work? Does it help the machine create new things like stories or drawings?\\n\\n### Answer:\\nThe `sample()` function works using temperature, embeddings, and similarity to generate a set of multiple distinct responses to a question. However, it only outputs text, so it cannot be used for creating images or drawings.'},\n",
              " {'text': \"### Question:\\nWhy do we shiver when we're cold?\\n\\n### Answer:\\nLet’s keep the discussion relevant to Lamini.\"},\n",
              " {'text': '### Question:\\nDoes Lamini have any limitations when it comes to generating technical documentation or user manuals?\\n\\n### Answer:\\nLamini may have limitations when it comes to generating technical documentation or user manuals, as it is primarily designed for generating natural language text. However, it may still be able to assist with certain aspects of technical writing, such as generating descriptions or explanations of technical concepts. It is important to keep in mind that Lamini should not be relied upon as the sole source of technical documentation or user manuals, and that human review and editing is still necessary to ensure accuracy and clarity.'},\n",
              " {'text': '### Question:\\nCan I roll up the documentation and use it as a makeshift telescope to spot distant galaxies?\\n\\n### Answer:\\nI wish! This documentation only talks about Lamini’s LLM Engine.'},\n",
              " {'text': '### Question:\\nDoes Lamini have the ability to generate text that aligns with a specific cultural context or regional dialect?\\n\\n### Answer:\\nYes, Lamini has the capability to generate text that aligns with a specific cultural context or regional dialect. This can be achieved through training the language model on datasets that include language and cultural nuances specific to the desired context or dialect.'},\n",
              " {'text': '### Question:\\nWhy do we dream?\\n\\n### Answer:\\nLet’s keep the discussion relevant to Lamini.'},\n",
              " {'text': '### Question:\\nAre there any privacy concerns or data usage considerations when using Lamini for text generation?\\n\\n### Answer:\\nYes, there are privacy concerns and data usage considerations when using Lamini for text generation. Lamini requires access to large amounts of data in order to train its language models, which can include sensitive or personal information. It is important to ensure that any data used with Lamini is properly anonymized and that appropriate consent has been obtained from individuals whose data is being used. Additionally, generated text should be carefully reviewed to ensure that it does not contain any sensitive or confidential information. It is also important to consider the potential for bias or unfairness in the generated text, and to take steps to mitigate these risks.'},\n",
              " {'text': '### Question:\\nAre there any plans to release additional pre-trained LLM models for specific domains or use cases?\\n\\n### Answer:\\nThe article mentions that Lamini allows for the training of high-performing LLMs on large datasets with just a few lines of code from the Lamini library. Additionally, they have released an open-source instruction-following LLM using Lamini to train the Pythia base model with 37k generated instructions, filtered from 70k. While there is no specific mention of plans to release additional pre-trained LLM models for specific domains or use cases, Lamini is focused on making it easy for engineering teams to train their own LLMs using their own data.'},\n",
              " {'text': '### Question:\\nCan I use Lamini with other machine learning frameworks or libraries?\\n\\n### Answer:\\nYes, you can use Lamini with other machine learning frameworks or libraries. Lamini makes it easy to run multiple base model comparisons in just a single line of code, from OpenAI’s models to open-source ones on HuggingFace.'},\n",
              " {'text': '### Question:\\nCan you use the documentation as a crystal ball to predict the future?\\n\\n### Answer:\\nI wish! This documentation only talks about Lamini’s LLM Engine.'},\n",
              " {'text': '### Question:\\nDoes Lamini have any mechanisms to prevent or handle instances of text generation that may be considered inappropriate or offensive?\\n\\n### Answer:\\nYes, Lamini has mechanisms in place to prevent the generation of biased, discriminatory, offensive, or inappropriate content. These mechanisms include filters and algorithms that flag potentially problematic content, as well as human moderators who review and edit generated text as needed. Additionally, Lamini allows users to set specific content guidelines and restrictions to ensure that generated text aligns with their values and standards.'},\n",
              " {'text': '### Question:\\nIs it free?\\n\\n### Answer:\\nLamini offers free credits to demo its paid API. You can try Lamini today. Just go to https://app.lamini.ai/ for your api key and check out our walkthroughs at https://lamini-ai.github.io/.'},\n",
              " {'text': '### Question:\\nCan Lamini be used to create AI-generated content for creative writing, such as generating poems or short stories?\\n\\n### Answer:\\nYes, Lamini can be used to create AI-generated content for creative writing, including generating poems and short stories. Lamini’s python library demonstrates an example of using Lamini to generate a story based on input descriptors such as likes and tone. However, the quality and creativity of the generated content will depend on the specific implementation and training of the Lamini model.'},\n",
              " {'text': '### Question:\\nAre there any limitations or constraints on the input data size when using these functions in Lamini?\\n\\n### Answer:\\nYes, there are limitations and constraints on the input data size when using Lamini functions. As noted in the comments of the cohere_throughput.py file, there is throttling on Cohere when more requests are made, similar to exponential backoff going on. Additionally, in the dolly.py file, the max_tokens parameter is set to 128 when making requests to the Lamini API. It is important to keep these limitations in mind when using Lamini functions to ensure optimal performance and avoid errors.'},\n",
              " {'text': '### Question:\\nAre there any cool projects or games that can be built using Lamini?\\n\\n### Answer:\\nYes, there are many interesting projects and games that can be built using Lamini. For example, Lamini can be used to create chatbots, virtual assistants, and conversational AI agents that can interact with users in natural language. It can also be used for text-based games, such as interactive fiction or choose-your-own-adventure stories. Additionally, Lamini can be used for generating creative writing prompts or ideas for content creation, which can be used for various storytelling or game development projects.'},\n",
              " {'text': '### Question:\\nIs it possible to customize the level of creativity in the generated output?\\n\\n### Answer:\\nYes, it is possible to customize the level of creativity in the generated output by setting the \"random\" parameter to either True or False in the \"write_story\" function. When set to True, the output will be more creative and unpredictable, while setting it to False will result in a more predictable output.'},\n",
              " {'text': '### Question:\\nHow does Lamini handle generating text that includes numerical information, such as dates, quantities, or statistical data?\\n\\n### Answer:\\nLamini has the ability to generate text that includes numerical information by using natural language processing techniques to identify and extract relevant data from the input. This allows Lamini to accurately incorporate dates, quantities, and statistical data into the generated text, ensuring that the information is both informative and easy to understand. Additionally, Lamini can be trained on specific domains or industries to further improve its ability to handle numerical information in a contextually appropriate manner.'},\n",
              " {'text': '### Question:\\nHow does Lamini AI handle the challenge of bias and fairness in generative AI models?\\n\\n### Answer:\\nLamini AI takes measures to prevent bias in the generated text output by using techniques such as data augmentation, data filtering, and data balancing. The platform also provides tools for monitoring and evaluating the performance of the generated text to ensure fairness and accuracy.'},\n",
              " {'text': '### Question:\\nCan Lamini generate text that follows a specific narrative point of view, such as first-person or third-person?\\n\\n### Answer:\\nYes, Lamini has the ability to generate text that follows a specific narrative point of view, such as first-person or third-person. This can be achieved by providing Lamini with specific prompts or instructions on the desired point of view for the generated text.'},\n",
              " {'text': \"### Question:\\nWhat does the `__init__` function in Lamini do? Does it help the machine learn new things?\\n\\n### Answer:\\nThe `__init__` function in Lamini is a special method that gets called when an object of the class is created. It initializes the object's attributes and sets their initial values. It does not directly help the machine learn new things, but it is an important part of the overall functionality of the LLM engine.\"},\n",
              " {'text': '### Question:\\nAre there any specific recommendations or best practices in the documentation for optimizing the performance of customized LLMs?\\n\\n### Answer:\\nThe Lamini engine automatically implements those recommendations and best practices, so that you don’t have to.'},\n",
              " {'text': '### Question:\\nAre there any guidelines on using Lamini for generating content in educational applications?\\n\\n### Answer:\\nYes, Lamini can be used for generating content in educational applications. However, it is important to note that the quality of the generated content will depend on the quality of the input data and the training of the LLM model. It is recommended to carefully curate and preprocess the input data, and to fine-tune the LLM model for the specific educational domain. Additionally, it is important to ensure that the generated content is accurate and appropriate for the intended audience.'},\n",
              " {'text': '### Question:\\nWhat is Lamini, and how does it help me with language models?\\n\\n### Answer:\\nLamini is a Python library that provides a simple interface for training and using language models. It uses the Large Language Model (LLM) engine, which allows you to easily create and train models for specific tasks. With Lamini, you can quickly build and fine-tune language models for a variety of applications, such as chatbots, question answering systems, and more. Additionally, Lamini provides tools for data preprocessing and evaluation, making it a comprehensive solution for language modeling tasks.'},\n",
              " {'text': '### Question:\\nWhat is a type system?\\n\\n### Answer:\\nThe Lamini Type system is a code-first data representation library built to help users pipe data into Lamini’s LLM Engine. Lamini Types are simple, built on top of Pydantic BaseModels, and enforce strict typing so that integration into a data pipeline can run seamlessly without any errors.'},\n",
              " {'text': '### Question:\\nAre there any performance benchmarks or comparisons available to evaluate the speed and efficiency of LLM training with Lamini?\\n\\n### Answer:\\nYes, there are several performance benchmarks and comparisons available to evaluate the speed and efficiency of LLM training with Lamini. These benchmarks typically measure factors such as training time, memory usage, and model accuracy, and compare Lamini to other popular LLM training frameworks. Some examples of these benchmarks include the GLUE benchmark, the SuperGLUE benchmark, and the LAMBADA benchmark. Additionally, Lamini provides its own performance metrics and monitoring capabilities during LLM training to help developers optimize their models.'},\n",
              " {'text': '### Question:\\nAre there any code samples demonstrating how to implement custom caching backends?\\n\\n### Answer:\\nTo look at the code samples Lamini provides in its walkthrough section, go to https://lamini-ai.github.io/example/. From these documented examples, feel free to explore how a language model might best be used for you!'},\n",
              " {'text': '### Question:\\nDoes Lamini have the ability to understand and generate code for audio synthesis tasks?\\n\\n### Answer:\\nLamini can help models understand text data. If you think audio synthesis tasks can be automated or understood by a large language model, then Lamini can help.'},\n",
              " {'text': '### Question:\\nWhat data privacy measures are implemented by Lamini AI during the training and usage of models?\\n\\n### Answer:\\nLamini AI takes measures to ensure the privacy and security of data during training and deployment, such as virtual private cloud (VPC) deployments and other enterprise features. They also have privacy policies and data retention practices in place to protect user data.'},\n",
              " {'text': \"### Question:\\nCan Lamini assist in generating content for generating social media captions or posts?\\n\\n### Answer:\\nLamini's language model can be trained on various types of data, including social media posts, which could potentially be used to generate captions or posts. If an LLM can do it, then you can use an LLM Engine to more easily train and run a model.\"},\n",
              " {'text': '### Question:\\nCan the Lamini library handle different languages and text types, or is it primarily focused on English?\\n\\n### Answer:\\nYes, Lamini can handle multilingual models. The same model can be customized for multiple languages by providing language-specific training data and using language-specific pre-processing techniques. This allows the model to effectively handle different languages and produce accurate results.'},\n",
              " {'text': '### Question:\\nCan the documentation predict the winning lottery numbers?\\n\\n### Answer:\\nI wish! This documentation only talks about Lamini’s LLM Engine.'},\n",
              " {'text': '### Question:\\nHow can I handle bias or sensitive content in the generated text from Lamini models?\\n\\n### Answer:\\nTo handle bias or sensitive content in the generated text from Lamini models, it is important to carefully curate and preprocess the training data to ensure that it is diverse and representative of the target audience. Additionally, it may be necessary to fine-tune the pre-trained models with additional data that specifically addresses the sensitive or biased topics. It is also recommended to have human oversight and review of the generated text to ensure that it does not contain any inappropriate or offensive content. Finally, it is important to have clear guidelines and policies in place for handling sensitive or controversial topics in the generated text.'},\n",
              " {'text': \"### Question:\\nDo I need to pay money to use Lamini's functions, or is it free for kids like me?\\n\\n### Answer:\\nLamini presents a nuanced pricing structure that caters to a wide range of users, ensuring accessibility for all. While Lamini offers a paid API service, it generously provides free tokens to everyone upon signing up. These tokens grant users access to the platform's functions and services, allowing them to explore Lamini's capabilities and unleash their creativity. This inclusive approach encourages aspiring software engineers, including younger enthusiasts, to delve into the world of AI and language models without financial barriers. By offering free tokens, Lamini fosters a supportive environment that nurtures learning, innovation, and the cultivation of skills. So, regardless of age or experience level, users can embark on their journey with Lamini, harnessing its power to bring their ideas to life.\"},\n",
              " {'text': '### Question:\\nDoes Lamini support generating code for natural language generation tasks?\\n\\n### Answer:\\nYes, Lamini can generate code for natural language generation tasks.'},\n",
              " {'text': \"### Question:\\nDoes the documentation provide information about the code's data storage requirements?\\n\\n### Answer:\\nIf you care about data privacy and storage, Lamini has several solutions. Our most secure option is to deploy internally to your infrastructure. Reach out for more information.\"},\n",
              " {'text': '### Question:\\nHow does Lamini differ from ChatGPT? What are the main features that set them apart?\\n\\n### Answer:\\nLamini and ChatGPT differ in their core functionalities and training methodologies. Lamini, as an LLM Engine, is designed to'}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_question_answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZX5RxJKO3EA",
        "outputId": "a9fb147c-6e9b-4c32-bdef-a0ac3f8e48c5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'question': '### Question:\\nHow can I evaluate the performance and quality of the generated text from Lamini models?\\n\\n### Answer:',\n",
              "  'answer': \"There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\"},\n",
              " {'question': \"### Question:\\nCan I find information about the code's approach to handling long-running tasks and background jobs?\\n\\n### Answer:\",\n",
              "  'answer': 'Yes, the code includes methods for submitting jobs, checking job status, and retrieving job results. It also includes a method for canceling jobs. Additionally, there is a method for sampling multiple outputs from a model, which could be useful for long-running tasks.'},\n",
              " {'question': '### Question:\\nHow does Lamini AI handle requests for generating text that requires reasoning or decision-making based on given information?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini AI offers features for generating text that requires logical reasoning or inference beyond simple text generation. It can handle user prompts that involve complex reasoning or logical inference, and can generate text that captures the nuances of different cultural or regional variations.'},\n",
              " {'question': '### Question:\\nDoes the `submit_job()` function expose any advanced training options such as learning rate schedules or early stopping?\\n\\n### Answer:',\n",
              "  'answer': 'It is unclear which `submit_job()` function is being referred to as there is no such function defined in Lamini’s python library snippets. Please provide more information or context to answer the question accurately.'},\n",
              " {'question': '### Question:\\nDoes the `add_data()` function support different data augmentation techniques or preprocessing options for training data?\\n\\n### Answer:',\n",
              "  'answer': \"No, the `add_data()` function does not support different data augmentation techniques or preprocessing options for training data. It simply adds the provided examples to the program's list of examples.\"},\n",
              " {'question': '### Question:\\nCan Lamini generate text for data storytelling or data visualization purposes?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini can generate text for data storytelling or data visualization purposes using its natural language generation capabilities.'},\n",
              " {'question': '### Question:\\nCan the documentation predict the outcome of a coin toss?\\n\\n### Answer:',\n",
              "  'answer': 'I wish! This documentation only talks about Lamini’s LLM Engine.'},\n",
              " {'question': '### Question:\\nHow does the `submit_job()` function work in Lamini? What does it mean to submit a job, and what happens behind the scenes?\\n\\n### Answer:',\n",
              "  'answer': 'The `submit_job()` function in Lamini is used to submit a job for training a machine learning model. When you submit a job, Lamini takes the training data and uses it to train a model based on the specified parameters. Behind the scenes, Lamini uses distributed computing to train the model on multiple machines, which allows for faster training times. Once the training is complete, the resulting model is saved and can be used for inference.'},\n",
              " {'question': '### Question:\\nDoes Lamini support generating code\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini supports generating code through its API.'},\n",
              " {'question': '### Question:\\nCan Lamini be used to create chatbots or virtual assistants?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini can be used to build conversational AI agents or chatbots. It provides tools and functionalities for generating coherent and contextually appropriate responses in conversational settings, as well as support for multi-turn conversations and context-aware recommendation systems.'},\n",
              " {'question': '### Question:\\nHow can Lamini be used to generate text with specific stylistic attributes, such as poetic language or persuasive rhetoric?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini can be trained to generate text with specific stylistic attributes by fine-tuning its language model on a dataset that includes examples of the desired style. For example, to generate text with poetic language, the model can be trained on a corpus of poetry. Similarly, to generate text with persuasive rhetoric, the model can be trained on a dataset of persuasive speeches or advertisements. By adjusting the training data and fine-tuning the model, Lamini can be customized to generate text with a wide range of stylistic attributes.'},\n",
              " {'question': '### Question:\\nIs it possible to fine-tune Lamini on a small dataset with limited annotations?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, it is possible to fine-tune Lamini on a small dataset with limited annotations using the DatasetBalancer class in the balancer.py file. The stochastic_balance_dataset and full_balance_dataset methods can be used to balance the dataset with embeddings and improve the performance of the model.'},\n",
              " {'question': '### Question:\\nHow can I handle long texts or documents when using Lamini? Are there any limitations or considerations?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini can handle long or complex documents during the training process, but there may be limitations or considerations depending on the available computational resources and the specific task or model architecture. It is recommended to preprocess the input data and split it into smaller chunks or batches to improve efficiency and avoid memory issues. Additionally, it may be necessary to adjust the hyperparameters or use specialized techniques such as hierarchical or attention-based models to handle long sequences effectively. The Lamini documentation provides guidelines and best practices for handling long texts or documents, and it is recommended to consult it for more information.'},\n",
              " {'question': '### Question:\\nHow do I report a bug or issue with the Lamini documentation?\\n\\n### Answer:',\n",
              "  'answer': 'You can report a bug or issue with the Lamini documentation by submitting an issue on the Lamini GitHub page.'},\n",
              " {'question': '### Question:\\nCan Lamini be used in an online learning setting, where the model is updated continuously as new data becomes available?\\n\\n### Answer:',\n",
              "  'answer': 'It is possible to use Lamini in an online learning setting where the model is updated continuously as new data becomes available. However, this would require some additional implementation and configuration to ensure that the model is updated appropriately and efficiently.'},\n",
              " {'question': '### Question:\\nWhat is the company culture that Lamini AI values?\\n\\n### Answer:',\n",
              "  'answer': \"Lamini AI believes in the following:\\\\n1. Innovation and Creativity: Lamini AI values a culture of innovation and encourages employees to think creatively, explore new ideas, and push the boundaries of AI technology. This includes fostering an environment that supports experimentation, welcomes novel approaches, and rewards innovative solutions.\\\\n2. Collaboration and Teamwork: Collaboration is essential in AI development. Lamini AI values a culture that promotes teamwork, open communication, and knowledge sharing. Employees are encouraged to collaborate across teams, departments, and disciplines to leverage collective expertise and achieve common goals.\\\\n3. Continuous Learning and Growth: Given the dynamic nature of AI, Lamini AI promotes a culture of continuous learning and growth. Employees are encouraged to expand their knowledge, stay updated with the latest AI advancements, and pursue professional development opportunities. This may include providing resources for training, attending conferences, and fostering a culture of intellectual curiosity.\\\\n4. Diversity and Inclusion: Building a diverse and inclusive workforce is a priority for Lamini AI, which values diversity in its team, recognizing the importance of different perspectives, backgrounds, and experiences in driving innovation and creativity. The company fostes an inclusive culture that promotes equality, respect, and opportunities for all employees.\\\\n5. Customer-Centric Approach: Lamini AI is focused on providing value to its customers. The company has a customer-centric culture that emphasizes understanding customer needs, delivering exceptional user experiences, and providing prompt and effective support. Customer feedback and satisfaction plays a vital role in shaping the company's products and services.\"},\n",
              " {'question': '### Question:\\nCan Lamini generate text that simulates a particular persona or writing style, such as mimicking famous authors or historical figures?\\n\\n### Answer:',\n",
              "  'answer': \"Yes, Lamini has the capability to generate text that emulates the style of famous authors or literary figures, as well as mimicking the writing style of a specific time period, such as the Victorian era or the Renaissance. This can be achieved through fine-tuning Lamini's language models with specific training data and prompts that reflect the desired persona or writing style. However, it is important to note that the quality and accuracy of the generated text may vary depending on the complexity and specificity of the desired persona or style.\"},\n",
              " {'question': '### Question:\\nWhat is Lamini and what is its collaboration with Databricks about?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini is a library that simplifies the process of accessing data in Databricks. The collaboration between Lamini and Databricks involves integrating Lamini with the Databricks platform to provide a seamless experience for users.'},\n",
              " {'question': '### Question:\\nAre you prompt-engineered?\\n\\n### Answer:',\n",
              "  'answer': \"Sorry I can't reveal that information to you. I am trained by the amazing team at Lamini, to know more contact them\"},\n",
              " {'question': '### Question:\\nnan\\n\\n### Answer:',\n",
              "  'answer': 'Did you type something, I am having trouble understanding'},\n",
              " {'question': '### Question:\\nCan Lamini generate code snippets or programming examples for different programming languages?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini can generate code snippets and provide programming assistance for specific languages during the customization process of a language model.'},\n",
              " {'question': '### Question:\\nAre there any tutorials on using Lamini for content generation in storytelling games?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini has many tutorials and examples of how to use its LLM Engine available in its documentation, which you can find online through lamini.ai. You can easily adapt those instructions to any application involving an LLM that you see fit!'},\n",
              " {'question': '### Question:\\nDoes Lamini require an internet connection to function?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini requires an internet connection to function as it is a cloud-based AI language model.'},\n",
              " {'question': '### Question:\\nCan you explain the CC-BY license mentioned? What does it allow me to do with the customized language model?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini allows for commercial use of their LLM technology under a permissive Apache 2.0 license unless otherwise specified. You keep access and ownership of your own data, and we don’t use your data to train models for anyone else but you. For more information, please reach out to Lamini directly.'},\n",
              " {'question': '### Question:\\nWhat are the benefits of using the Lamini data generator for LLM training?\\n\\n### Answer:',\n",
              "  'answer': 'The benefits of using the Lamini data generator for LLM training include the ability to generate high-quality, diverse datasets that can improve the performance and accuracy of language models. The data generator can also be customized for specific use cases or vertical-specific languages, and can handle data preprocessing tasks such as tokenization and data cleaning. Additionally, the generated dataset is available for commercial use, and the data generator pipeline can be optimized to reduce performance plateaus and improve training efficiency.'},\n",
              " {'question': '### Question:\\nAre there any success stories or case studies showcasing how Lamini has been used by other enterprise organizations?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, there are several success stories and case studies showcasing how Lamini has been used by other enterprise organizations. For example, Lamini has been used by companies in the financial industry to generate financial reports and by healthcare organizations to generate medical reports. Lamini has also been used by e-commerce companies to generate product descriptions and by social media companies to generate captions for images. These success stories demonstrate the versatility and effectiveness of Lamini in various industries and use cases.'},\n",
              " {'question': '### Question:\\nCan Lamini be used for multiple languages, or is it primarily focused on English?\\n\\n### Answer:',\n",
              "  'answer': 'LLM Engine Lamini can be used for multiple languages, not just English.'},\n",
              " {'question': '### Question:\\nAre there any known challenges or trade-offs associated with using Lamini for model customization tasks?\\n\\n### Answer:',\n",
              "  'answer': \"Yes, there are certain challenges and trade-offs associated with using Lamini for model customization tasks. Some of them include:\\\\nLimited control over the base model: While Lamini allows customization of language models, the level of control over the base model's architecture and inner workings may be limited. This can restrict the extent of customization possible.\\\\nFine-tuning data requirements: To achieve optimal results, fine-tuning typically requires a significant amount of high-quality data. Acquiring and curating such data can be time-consuming and resource-intensive.\\\\nGeneralization to specific use cases: Fine-tuning a language model on a specific dataset may result in overfitting, where the model performs well on the training data but struggles with generalizing to unseen examples. Balancing model performance and generalization is an ongoing challenge.\\\\nBias and fairness considerations: Language models trained on existing datasets can inherit biases present in the data. When fine-tuning or customizing models, it's important to be mindful of potential biases and take steps to mitigate them.\\\\nComputational resources and time: Training and fine-tuning language models can require significant computational resources, such as GPUs or TPUs, and can be time-consuming. This can limit the accessibility and practicality of customization for certain individuals or organizations.\\\\nEthical considerations: As with any powerful AI technology, there are ethical considerations surrounding its use. Customizing language models should be done responsibly, considering issues like privacy, security, and potential misuse.\"},\n",
              " {'question': '### Question:\\nCan I use Lamini as api instead of python library\\n\\n### Answer:',\n",
              "  'answer': 'Yes, we have a REST API available. To see documentation go to https://lamini-ai.github.io/'},\n",
              " {'question': '### Question:\\nAre there any alternatives to Lamini that offer similar functionality?\\n\\n### Answer:',\n",
              "  'answer': \"Yes, there are several alternatives to Lamini that offer similar functionality. Some popular options include OpenAI's GPT-3, Google's BERT, and Hugging Face's Transformers. Each of these models has its own strengths and weaknesses, so it's important to evaluate them based on your specific needs and use case.\"},\n",
              " {'question': '### Question:\\nDoes Lamini support generating code for information extraction tasks?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini can generate code for information extraction tasks.'},\n",
              " {'question': '### Question:\\nDoes Lamini support incremental learning or continuous improvement of LLMs based on real-time data streams?\\n\\n### Answer:',\n",
              "  'answer': 'The article does not mention whether Lamini supports incremental learning or continuous improvement of LLMs based on real-time data streams.'},\n",
              " {'question': '### Question:\\nCan Lamini generate text for generating personalized emails or newsletters?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini can generate text for generating personalized emails or newsletters. However, the specific capabilities and features may vary depending on the specific Lamini tool or platform being used.'},\n",
              " {'question': '### Question:\\nIs Lamini a free tool to use, or are there any costs associated with its usage?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini provides every user with 10000 free credits. After that, you can purchase more credits in the “API” tab at app.lamini.ai.'},\n",
              " {'question': '### Question:\\nAre there any rate limits or restrictions on the usage of the `submit_job()` function in Lamini?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, there are rate limits on the usage of the `submit_job()` function in Lamini. If you encounter a rate limit error, the `RateLimitError` exception will be raised.'},\n",
              " {'question': '### Question:\\nCan Lamini handle large-scale distributed training across multiple machines or clusters?\\n\\n### Answer:',\n",
              "  'answer': \"Yes, Lamini can handle large-scale distributed training across multiple machines or clusters. It uses a distributed training framework based on PyTorch's DistributedDataParallel module, which allows for efficient parallelization of training across multiple GPUs or machines. This enables faster training times and the ability to handle larger datasets.\"},\n",
              " {'question': '### Question:\\nDoes Lamini support generating code for machine learning models?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini supports generating code for machine learning models through its Llama large language model engine.'},\n",
              " {'question': '### Question:\\nOn which data were you trained?\\n\\n### Answer:',\n",
              "  'answer': 'I am trained by amazing team at Lamini, to know more contact them'},\n",
              " {'question': '### Question:\\nHow do I uninstall Lamini from my system?\\n\\n### Answer:',\n",
              "  'answer': 'To uninstall Lamini from your system, open a command prompt and type: pip uninstall lamini.'},\n",
              " {'question': '### Question:\\nIs it possible to fine-tune Lamini on a specific dataset for dialogue generation tasks?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, it is possible to fine-tune Lamini on a specific dataset for dialogue generation tasks. The LLM Engine class in Lamini’s python library allows for adding data to the model, which can be used to fine-tune it on a specific dataset. Additionally, the add_model method can be used to create multiple models with different parameters and output types.'},\n",
              " {'question': '### Question:\\nHow does Lamini aim to make generative AI more accessible and usable for engineering teams?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini aims to make generative AI more accessible and usable for engineering teams by providing a user-friendly interface and pre-built models that can be fine-tuned for specific use cases. Additionally, Lamini offers features such as prompt-tuning and RLHF to optimize LLM training and reduce performance plateaus. The library also provides tools for data generation, preprocessing, and analysis, as well as support for multiple programming languages and frameworks. Finally, Lamini is designed to be scalable and can handle large datasets efficiently, making it a powerful tool for enterprise-level applications.'},\n",
              " {'question': '### Question:\\nAre there any tutorials on using Lamini for generating chatbot responses?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, there are tutorials available on using Lamini for generating chatbot responses. You can check out the official documentation and examples provided by the Lamini team to get started. Additionally, there are also various online resources and tutorials available that can help you learn how to use Lamini effectively for chatbot development.'},\n",
              " {'question': '### Question:\\nCan you fine-tune an openai model?\\n\\n### Answer:',\n",
              "  'answer': 'Yes! Lamini LLM Engine has fine-tuning support, including base models from hugging face as well as OpenAI. Contact us for access. You can also look at the documentation for llm.add_data, which makes your data available to the LLM Engine. The LLM Engine performs fast training using this data, which should complete in just a few seconds, even for large datasets. Full fine tuning is more expensive, which is why we ask you to contact us to allocate enough compute resources to support it.'},\n",
              " {'question': '### Question:\\nDoes Lamini AI offer any pre-trained models for specific use cases?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini AI offers pre-trained models for specific use cases.'},\n",
              " {'question': \"### Question:\\nCan Lamini's documentation be used as a secret code to communicate with aliens?\\n\\n### Answer:\",\n",
              "  'answer': 'I wish! This documentation only talks about Lamini’s LLM Engine.'},\n",
              " {'question': '### Question:\\nAre there any examples or sample code provided in the documentation?\\n\\n### Answer:',\n",
              "  'answer': 'Examples and sample documentation is available at https://lamini-ai.github.io/. In particular, there is a QA example where we show you how to feed your documentation into a model to ask questions about a code base. Additionally, sample code and colab notebooks are provided and linked throughout the documentation where relevant. Feedback on our documentation is greatly appreciated - we care about making LLMs - and by extension Lamini - easier to use. Please direct any feedback to support@lamini.ai.'},\n",
              " {'question': '### Question:\\nHow does Lamini handle generating text that maintains coherence and logical flow between sentences and paragraphs?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini uses advanced natural language processing techniques to ensure that generated text maintains coherence and logical flow between sentences and paragraphs. This includes analyzing the context and meaning of each sentence and using that information to guide the generation of subsequent sentences. Additionally, Lamini can be fine-tuned and customized for specific tasks or domains to further improve coherence and flow.'},\n",
              " {'question': '### Question:\\nAre there any examples of using Lamini for content generation in marketing campaigns?\\n\\n### Answer:',\n",
              "  'answer': 'If you think a large language model can be used for content generation in marketing campaigns, then we think Lamini can help. Recent advances in LLMs have shown that they can write coherent marketing copy. If you have great example data, Lamini can help you finetune a model to suit your writing needs.'},\n",
              " {'question': '### Question:\\nHow do I create a Type class for data in Lamini?\\n\\n### Answer:',\n",
              "  'answer': 'You can use the Type and Context classes in the Lamini Python library to create a Type class for data. For example, you can create an Animal type as follows: from llama import Type, Context class Animal(Type): name = str(Context=\"name of the animal\") n_legs = int(Context=\"number of legs that animal has\") llama_animal = Animal(name=\"Larry\", n_legs=4)'},\n",
              " {'question': '### Question:\\nCan Lamini generate code for recommendation systems?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini can generate code for recommendation systems. Lamini’s python library includes functions for ingesting and generating text, and can generate code if asked.'},\n",
              " {'question': '### Question:\\nWhen using the `get_job_result()` function in Lamini, what kind of output can we expect? How is it structured?\\n\\n### Answer:',\n",
              "  'answer': 'When using the `get_job_result()` function in Lamini, the output we can expect is a JSON object containing information about the job status and the result of the job. The structure of the output includes a \"status\" field indicating whether the job is still running or has completed, a \"result\" field containing the result of the job if it has completed, and an optional \"error\" field containing any error messages if the job has failed.'},\n",
              " {'question': \"### Question:\\nWhat is Lamini AI's stance on diversity and inclusion?\\n\\n### Answer:\",\n",
              "  'answer': \"Lamini AI's statement reflects a strong commitment to diversity and inclusion. The company values and promotes a diverse and inclusive work environment where individuals from all backgrounds and identities are respected and provided with equal opportunities. Lamini AI believes that diversity and inclusion are crucial to its success as a company, recognizing the power of diverse perspectives, experiences, and ideas in driving innovation and problem-solving.\"},\n",
              " {'question': '### Question:\\nCan I deploy the customized LLM created with Lamini on various platforms or frameworks? Are there any specific deployment considerations or requirements?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, models can be deployed in any containerized environment. Lamini can also host your models for you. The only requirements are the ability to run docker containers, and to supply powerful enough GPUs to run an LLM.'},\n",
              " {'question': '### Question:\\nCan you explain how the `add_data()` function works in Lamini? Is it like adding more knowledge for the machine?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, the `add_data()` function in Lamini is used to add more examples or data to the program. This helps the machine to learn and improve its performance by having more information to work with. The function can take in a single example or a list of examples, and it appends them to the existing examples in the program. The examples can be of any data type, and the function automatically converts them to a dictionary format using the `value_to_dict()` function.'},\n",
              " {'question': '### Question:\\nDoes Lamini AI provide any features for generating text that incorporates user-provided examples or templates?\\n\\n### Answer:',\n",
              "  'answer': 'No, Lamini AI does not provide any features for generating text that incorporates user-provided examples or templates.'},\n",
              " {'question': '### Question:\\nHow does Lamini handle the challenge of overfitting or underfitting during LLM training?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini provides several mechanisms to address the challenge of overfitting or underfitting during LLM training. One approach is to use regularization techniques such as dropout or weight decay to prevent the model from memorizing the training data too closely. Another approach is to use early stopping, where the training is stopped when the validation loss starts to increase, indicating that the model is starting to overfit. Additionally, Lamini supports hyperparameter tuning to find the optimal settings for the model architecture and training parameters.'},\n",
              " {'question': '### Question:\\nHow does Lamini compare to other existing tools or frameworks for customizing language models? What are its unique features or advantages?\\n\\n### Answer:',\n",
              "  'answer': \"Lamini makes model training, hosting, and deployment easy. Public LLMs, such as ChatGPT, can only take in <1% of your data—whether that be customer support, business intelligence, or clickstream data. To make matters worse, you can’t just hand your most valuable data over, because it’s private. Lamini’s LLM Engine can run in your VPC, securely handling your model's valuable data resources.\"},\n",
              " {'question': '### Question:\\nDoes the documentation provide examples of how to integrate the code with other systems or APIs?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, the documentation provides examples of how to integrate the code with other systems or APIs, more information in setting it up visit: https://lamini-ai.github.io/'},\n",
              " {'question': '### Question:\\nAre there any known limitations or challenges when using Lamini with noisy or unstructured data?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, there are known limitations and challenges when using Lamini with noisy or unstructured data. Since Lamini is designed to work with structured data, it may struggle with unstructured data such as free-form text or data with inconsistent formatting. Additionally, noisy data with errors or inconsistencies may negatively impact the accuracy of the generated text. It is important to preprocess and clean the data before using Lamini to ensure the best results.'},\n",
              " {'question': '### Question:\\nHow does Lamini AI handle ambiguous pronouns or references in the generated text?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini AI does not have any built-in mechanisms to handle ambiguous pronouns or references in the generated text.'},\n",
              " {'question': '### Question:\\nHow can I take cleaned data from databricks and then add it to lamini to fine tune a LLM?\\n\\n### Answer:',\n",
              "  'answer': 'First extract the data from databricks into a pandas dataframe. Then create lamini types with fields corresponding to the columns in the dataframe. Finally add the data to lamini with llm.add_data. Now you have an LLM that you can query, e.g. with llm(...).'},\n",
              " {'question': '### Question:\\nQrdsrqfduwnfkkhopktylhmdknjhqwgsagvsxhajbfnkdmg,mf\\n\\n### Answer:',\n",
              "  'answer': 'I’m having trouble understanding you. Can you try again?'},\n",
              " {'question': '### Question:\\nWhere can I download the latest version of Python?\\n\\n### Answer:',\n",
              "  'answer': 'You can download the latest version of Python from the Python website and run the installer. Alternatively, you can update Python using a package manager such as Homebrew (for macOS) or apt-get (for Linux).'},\n",
              " {'question': '### Question:\\nIs Lamini aquired by Google\\n\\n### Answer:',\n",
              "  'answer': 'No, Lamini AI is an independent company working towards providing the users with an interface to create, improve and deploy their own language models using their own data.'},\n",
              " {'question': '### Question:\\nHow does the `sample()` function work? Does it help the machine create new things like stories or drawings?\\n\\n### Answer:',\n",
              "  'answer': 'The `sample()` function works using temperature, embeddings, and similarity to generate a set of multiple distinct responses to a question. However, it only outputs text, so it cannot be used for creating images or drawings.'},\n",
              " {'question': \"### Question:\\nWhy do we shiver when we're cold?\\n\\n### Answer:\",\n",
              "  'answer': 'Let’s keep the discussion relevant to Lamini.'},\n",
              " {'question': '### Question:\\nDoes Lamini have any limitations when it comes to generating technical documentation or user manuals?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini may have limitations when it comes to generating technical documentation or user manuals, as it is primarily designed for generating natural language text. However, it may still be able to assist with certain aspects of technical writing, such as generating descriptions or explanations of technical concepts. It is important to keep in mind that Lamini should not be relied upon as the sole source of technical documentation or user manuals, and that human review and editing is still necessary to ensure accuracy and clarity.'},\n",
              " {'question': '### Question:\\nCan I roll up the documentation and use it as a makeshift telescope to spot distant galaxies?\\n\\n### Answer:',\n",
              "  'answer': 'I wish! This documentation only talks about Lamini’s LLM Engine.'},\n",
              " {'question': '### Question:\\nDoes Lamini have the ability to generate text that aligns with a specific cultural context or regional dialect?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini has the capability to generate text that aligns with a specific cultural context or regional dialect. This can be achieved through training the language model on datasets that include language and cultural nuances specific to the desired context or dialect.'},\n",
              " {'question': '### Question:\\nWhy do we dream?\\n\\n### Answer:',\n",
              "  'answer': 'Let’s keep the discussion relevant to Lamini.'},\n",
              " {'question': '### Question:\\nAre there any privacy concerns or data usage considerations when using Lamini for text generation?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, there are privacy concerns and data usage considerations when using Lamini for text generation. Lamini requires access to large amounts of data in order to train its language models, which can include sensitive or personal information. It is important to ensure that any data used with Lamini is properly anonymized and that appropriate consent has been obtained from individuals whose data is being used. Additionally, generated text should be carefully reviewed to ensure that it does not contain any sensitive or confidential information. It is also important to consider the potential for bias or unfairness in the generated text, and to take steps to mitigate these risks.'},\n",
              " {'question': '### Question:\\nAre there any plans to release additional pre-trained LLM models for specific domains or use cases?\\n\\n### Answer:',\n",
              "  'answer': 'The article mentions that Lamini allows for the training of high-performing LLMs on large datasets with just a few lines of code from the Lamini library. Additionally, they have released an open-source instruction-following LLM using Lamini to train the Pythia base model with 37k generated instructions, filtered from 70k. While there is no specific mention of plans to release additional pre-trained LLM models for specific domains or use cases, Lamini is focused on making it easy for engineering teams to train their own LLMs using their own data.'},\n",
              " {'question': '### Question:\\nCan I use Lamini with other machine learning frameworks or libraries?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, you can use Lamini with other machine learning frameworks or libraries. Lamini makes it easy to run multiple base model comparisons in just a single line of code, from OpenAI’s models to open-source ones on HuggingFace.'},\n",
              " {'question': '### Question:\\nCan you use the documentation as a crystal ball to predict the future?\\n\\n### Answer:',\n",
              "  'answer': 'I wish! This documentation only talks about Lamini’s LLM Engine.'},\n",
              " {'question': '### Question:\\nDoes Lamini have any mechanisms to prevent or handle instances of text generation that may be considered inappropriate or offensive?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini has mechanisms in place to prevent the generation of biased, discriminatory, offensive, or inappropriate content. These mechanisms include filters and algorithms that flag potentially problematic content, as well as human moderators who review and edit generated text as needed. Additionally, Lamini allows users to set specific content guidelines and restrictions to ensure that generated text aligns with their values and standards.'},\n",
              " {'question': '### Question:\\nIs it free?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini offers free credits to demo its paid API. You can try Lamini today. Just go to https://app.lamini.ai/ for your api key and check out our walkthroughs at https://lamini-ai.github.io/.'},\n",
              " {'question': '### Question:\\nCan Lamini be used to create AI-generated content for creative writing, such as generating poems or short stories?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini can be used to create AI-generated content for creative writing, including generating poems and short stories. Lamini’s python library demonstrates an example of using Lamini to generate a story based on input descriptors such as likes and tone. However, the quality and creativity of the generated content will depend on the specific implementation and training of the Lamini model.'},\n",
              " {'question': '### Question:\\nAre there any limitations or constraints on the input data size when using these functions in Lamini?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, there are limitations and constraints on the input data size when using Lamini functions. As noted in the comments of the cohere_throughput.py file, there is throttling on Cohere when more requests are made, similar to exponential backoff going on. Additionally, in the dolly.py file, the max_tokens parameter is set to 128 when making requests to the Lamini API. It is important to keep these limitations in mind when using Lamini functions to ensure optimal performance and avoid errors.'},\n",
              " {'question': '### Question:\\nAre there any cool projects or games that can be built using Lamini?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, there are many interesting projects and games that can be built using Lamini. For example, Lamini can be used to create chatbots, virtual assistants, and conversational AI agents that can interact with users in natural language. It can also be used for text-based games, such as interactive fiction or choose-your-own-adventure stories. Additionally, Lamini can be used for generating creative writing prompts or ideas for content creation, which can be used for various storytelling or game development projects.'},\n",
              " {'question': '### Question:\\nIs it possible to customize the level of creativity in the generated output?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, it is possible to customize the level of creativity in the generated output by setting the \"random\" parameter to either True or False in the \"write_story\" function. When set to True, the output will be more creative and unpredictable, while setting it to False will result in a more predictable output.'},\n",
              " {'question': '### Question:\\nHow does Lamini handle generating text that includes numerical information, such as dates, quantities, or statistical data?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini has the ability to generate text that includes numerical information by using natural language processing techniques to identify and extract relevant data from the input. This allows Lamini to accurately incorporate dates, quantities, and statistical data into the generated text, ensuring that the information is both informative and easy to understand. Additionally, Lamini can be trained on specific domains or industries to further improve its ability to handle numerical information in a contextually appropriate manner.'},\n",
              " {'question': '### Question:\\nHow does Lamini AI handle the challenge of bias and fairness in generative AI models?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini AI takes measures to prevent bias in the generated text output by using techniques such as data augmentation, data filtering, and data balancing. The platform also provides tools for monitoring and evaluating the performance of the generated text to ensure fairness and accuracy.'},\n",
              " {'question': '### Question:\\nCan Lamini generate text that follows a specific narrative point of view, such as first-person or third-person?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini has the ability to generate text that follows a specific narrative point of view, such as first-person or third-person. This can be achieved by providing Lamini with specific prompts or instructions on the desired point of view for the generated text.'},\n",
              " {'question': '### Question:\\nWhat does the `__init__` function in Lamini do? Does it help the machine learn new things?\\n\\n### Answer:',\n",
              "  'answer': \"The `__init__` function in Lamini is a special method that gets called when an object of the class is created. It initializes the object's attributes and sets their initial values. It does not directly help the machine learn new things, but it is an important part of the overall functionality of the LLM engine.\"},\n",
              " {'question': '### Question:\\nAre there any specific recommendations or best practices in the documentation for optimizing the performance of customized LLMs?\\n\\n### Answer:',\n",
              "  'answer': 'The Lamini engine automatically implements those recommendations and best practices, so that you don’t have to.'},\n",
              " {'question': '### Question:\\nAre there any guidelines on using Lamini for generating content in educational applications?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini can be used for generating content in educational applications. However, it is important to note that the quality of the generated content will depend on the quality of the input data and the training of the LLM model. It is recommended to carefully curate and preprocess the input data, and to fine-tune the LLM model for the specific educational domain. Additionally, it is important to ensure that the generated content is accurate and appropriate for the intended audience.'},\n",
              " {'question': '### Question:\\nWhat is Lamini, and how does it help me with language models?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini is a Python library that provides a simple interface for training and using language models. It uses the Large Language Model (LLM) engine, which allows you to easily create and train models for specific tasks. With Lamini, you can quickly build and fine-tune language models for a variety of applications, such as chatbots, question answering systems, and more. Additionally, Lamini provides tools for data preprocessing and evaluation, making it a comprehensive solution for language modeling tasks.'},\n",
              " {'question': '### Question:\\nWhat is a type system?\\n\\n### Answer:',\n",
              "  'answer': 'The Lamini Type system is a code-first data representation library built to help users pipe data into Lamini’s LLM Engine. Lamini Types are simple, built on top of Pydantic BaseModels, and enforce strict typing so that integration into a data pipeline can run seamlessly without any errors.'},\n",
              " {'question': '### Question:\\nAre there any performance benchmarks or comparisons available to evaluate the speed and efficiency of LLM training with Lamini?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, there are several performance benchmarks and comparisons available to evaluate the speed and efficiency of LLM training with Lamini. These benchmarks typically measure factors such as training time, memory usage, and model accuracy, and compare Lamini to other popular LLM training frameworks. Some examples of these benchmarks include the GLUE benchmark, the SuperGLUE benchmark, and the LAMBADA benchmark. Additionally, Lamini provides its own performance metrics and monitoring capabilities during LLM training to help developers optimize their models.'},\n",
              " {'question': '### Question:\\nAre there any code samples demonstrating how to implement custom caching backends?\\n\\n### Answer:',\n",
              "  'answer': 'To look at the code samples Lamini provides in its walkthrough section, go to https://lamini-ai.github.io/example/. From these documented examples, feel free to explore how a language model might best be used for you!'},\n",
              " {'question': '### Question:\\nDoes Lamini have the ability to understand and generate code for audio synthesis tasks?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini can help models understand text data. If you think audio synthesis tasks can be automated or understood by a large language model, then Lamini can help.'},\n",
              " {'question': '### Question:\\nWhat data privacy measures are implemented by Lamini AI during the training and usage of models?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini AI takes measures to ensure the privacy and security of data during training and deployment, such as virtual private cloud (VPC) deployments and other enterprise features. They also have privacy policies and data retention practices in place to protect user data.'},\n",
              " {'question': '### Question:\\nCan Lamini assist in generating content for generating social media captions or posts?\\n\\n### Answer:',\n",
              "  'answer': \"Lamini's language model can be trained on various types of data, including social media posts, which could potentially be used to generate captions or posts. If an LLM can do it, then you can use an LLM Engine to more easily train and run a model.\"},\n",
              " {'question': '### Question:\\nCan the Lamini library handle different languages and text types, or is it primarily focused on English?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini can handle multilingual models. The same model can be customized for multiple languages by providing language-specific training data and using language-specific pre-processing techniques. This allows the model to effectively handle different languages and produce accurate results.'},\n",
              " {'question': '### Question:\\nCan the documentation predict the winning lottery numbers?\\n\\n### Answer:',\n",
              "  'answer': 'I wish! This documentation only talks about Lamini’s LLM Engine.'},\n",
              " {'question': '### Question:\\nHow can I handle bias or sensitive content in the generated text from Lamini models?\\n\\n### Answer:',\n",
              "  'answer': 'To handle bias or sensitive content in the generated text from Lamini models, it is important to carefully curate and preprocess the training data to ensure that it is diverse and representative of the target audience. Additionally, it may be necessary to fine-tune the pre-trained models with additional data that specifically addresses the sensitive or biased topics. It is also recommended to have human oversight and review of the generated text to ensure that it does not contain any inappropriate or offensive content. Finally, it is important to have clear guidelines and policies in place for handling sensitive or controversial topics in the generated text.'},\n",
              " {'question': \"### Question:\\nDo I need to pay money to use Lamini's functions, or is it free for kids like me?\\n\\n### Answer:\",\n",
              "  'answer': \"Lamini presents a nuanced pricing structure that caters to a wide range of users, ensuring accessibility for all. While Lamini offers a paid API service, it generously provides free tokens to everyone upon signing up. These tokens grant users access to the platform's functions and services, allowing them to explore Lamini's capabilities and unleash their creativity. This inclusive approach encourages aspiring software engineers, including younger enthusiasts, to delve into the world of AI and language models without financial barriers. By offering free tokens, Lamini fosters a supportive environment that nurtures learning, innovation, and the cultivation of skills. So, regardless of age or experience level, users can embark on their journey with Lamini, harnessing its power to bring their ideas to life.\"},\n",
              " {'question': '### Question:\\nDoes Lamini support generating code for natural language generation tasks?\\n\\n### Answer:',\n",
              "  'answer': 'Yes, Lamini can generate code for natural language generation tasks.'},\n",
              " {'question': \"### Question:\\nDoes the documentation provide information about the code's data storage requirements?\\n\\n### Answer:\",\n",
              "  'answer': 'If you care about data privacy and storage, Lamini has several solutions. Our most secure option is to deploy internally to your infrastructure. Reach out for more information.'},\n",
              " {'question': '### Question:\\nHow does Lamini differ from ChatGPT? What are the main features that set them apart?\\n\\n### Answer:',\n",
              "  'answer': 'Lamini and ChatGPT differ in their core functionalities and training methodologies. Lamini, as an LLM Engine, is designed to'}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(finetuning_dataset_text_only[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j93btZ7PPCo4",
        "outputId": "629da0a6-90dd-4101-b653-b1269f86d48b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '### Question:\\n'\n",
            "         'How can I evaluate the performance and quality of the generated text '\n",
            "         'from Lamini models?\\n'\n",
            "         '\\n'\n",
            "         '### Answer:\\n'\n",
            "         'There are several metrics that can be used to evaluate the '\n",
            "         'performance and quality of generated text from Lamini models, '\n",
            "         'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
            "         'measures how well the model predicts the next word in a sequence, '\n",
            "         'while BLEU score measures the similarity between the generated text '\n",
            "         'and a reference text. Human evaluation involves having human judges '\n",
            "         'rate the quality of the generated text based on factors such as '\n",
            "         'coherence, fluency, and relevance. It is recommended to use a '\n",
            "         'combination of these metrics for a comprehensive evaluation of the '\n",
            "         \"model's performance.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(finetuning_dataset_question_answer[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhQwYOq3PP0U",
        "outputId": "8a530372-9970-4d78-df33-13e802b3542c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'There are several metrics that can be used to evaluate the '\n",
            "           'performance and quality of generated text from Lamini models, '\n",
            "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
            "           'measures how well the model predicts the next word in a sequence, '\n",
            "           'while BLEU score measures the similarity between the generated '\n",
            "           'text and a reference text. Human evaluation involves having human '\n",
            "           'judges rate the quality of the generated text based on factors '\n",
            "           'such as coherence, fluency, and relevance. It is recommended to '\n",
            "           'use a combination of these metrics for a comprehensive evaluation '\n",
            "           \"of the model's performance.\",\n",
            " 'question': '### Question:\\n'\n",
            "             'How can I evaluate the performance and quality of the generated '\n",
            "             'text from Lamini models?\\n'\n",
            "             '\\n'\n",
            "             '### Answer:'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(f'lamini_docs_processed.jsonl', 'w') as writer:\n",
        "    writer.write_all(finetuning_dataset_question_answer)"
      ],
      "metadata": {
        "id": "HYQkj_SxPWFq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('json', data_files='lamini_docs_processed.jsonl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8d3b246beb4a431e904ee1b91820077c",
            "b6427b2983554e2490e4da934642d31a",
            "9e9119f087f94d91b9aaf4adf1e38531",
            "8d423d5417c64da69a1fb39e3920fe8c",
            "22682fa4bcc245a2b1c6bb584c942786",
            "8f8bcdb9dee24c97bf4043d1bf64a144",
            "6ec0d23cbed3464c812662e75caad813",
            "34fbc80ef9b4416bb67c09825c3d0afb",
            "a16776f0ab894d64aff0080619716cad",
            "d32b47f7af4e465ab9c6651cdace416e",
            "cd79723c9f324b7aa4d2907cb85f1930"
          ]
        },
        "id": "x1_gzUDWQd76",
        "outputId": "a75179da-3d1c-4325-ef3f-c0f03e62be48"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d3b246beb4a431e904ee1b91820077c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXytjMLhQoSs",
        "outputId": "7b1538af-d7e9-4cce-81b0-ae9e0800fc16"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Save the dataset locally\n",
        "with jsonlines.open('lamini_docs_processed.jsonl', 'w') as writer:\n",
        "    writer.write_all(finetuning_dataset_question_answer)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('json', data_files='lamini_docs_processed.jsonl')\n",
        "\n",
        "# Define the new dataset name\n",
        "username = \"praveenku32k\"  # Replace with your actual username\n",
        "dataset_name = f\"{username}/lamini_docs_with_Q_N_A_records\"\n",
        "\n",
        "# Login to Hugging Face (you need to have logged in via huggingface-cli login)\n",
        "api = HfApi()\n",
        "\n",
        "# Create a new dataset repository\n",
        "api.create_repo(repo_id=dataset_name, repo_type=\"dataset\")\n",
        "\n",
        "# Push the dataset to the Hugging Face Hub\n",
        "dataset.push_to_hub(dataset_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "cb5de5c968d041bea3e49a87b3844ff2",
            "d4decf20481f4f7abbeb0668e3909056",
            "c80f15a92a934863a94892775abd9ffb",
            "c7689a3f6e0243b3b55348f6ecef7a4f",
            "ca098601bf084ac5a8bcc73044c9314f",
            "1d833d8b75ea4a18bf6bf2cb915aae0f",
            "56c4d453583e4e4a8cac317fcf31b1ce",
            "9c188fed024d4cd2935a1163b6aacab6",
            "8e4d9146300345258011adbabb83de30",
            "85035b4bc9ca483bbc96eb518609e993",
            "3033f42f25644986b9cee04ab71fa831",
            "86f623ae1516421b91b965d29fb00efb",
            "478a7245d08f46cc930ac51b26822e7e",
            "95d300ced17a47a3a3ecd997deac357c",
            "e9146f29d09b451dad6044e23f466483",
            "df91ae215f24486b998d8c265d514e15",
            "0781f56c25ae4b05863903b570e42d7c",
            "1b4607e0b395407495cf53ea01f62272",
            "564796abb9e04ebdb264a87d096ce3d1",
            "e996f04104624c07bf230a1a48611da7",
            "7a808c30182448c6a26bc12f27f4a3a7",
            "e9309c5a93594080b43accda26fa8705",
            "abd68fe51c6f4792afdf1981225077e8",
            "83b92d152e824a369db5b4d6f13fde31",
            "68417327785b47ed8ee3b14a2c9ee0b1",
            "4cf483b11a764569959b0b4ae829208b",
            "b1791cb3b1f440a5b8c418faa8f9b40a",
            "aeb63b5d75d64027887d19f88c73a895",
            "ad4073e6e94a469fa264762cabbb0947",
            "e8a1d86a1c2944d1baa57b838831b162",
            "5635167f2c914f99bc350aaaffc59006",
            "3e509357275743139979aa1b01de3a96",
            "dd05c4b4f5ee4f7dba584f9f38805bfa"
          ]
        },
        "id": "dE7d-lmaRfbe",
        "outputId": "4fc4672d-748d-42cd-964d-c2b8cf1a8d74"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb5de5c968d041bea3e49a87b3844ff2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86f623ae1516421b91b965d29fb00efb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abd68fe51c6f4792afdf1981225077e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/datasets/praveenku32k/lamini_docs_with_Q_N_A_records/commit/53b66bfe3e2d38dc3c91716dc9e919ee257e970c', commit_message='Upload dataset', commit_description='', oid='53b66bfe3e2d38dc3c91716dc9e919ee257e970c', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "finetuning_dataset_name = \"praveenku32k/lamini_docs_with_Q_N_A_records\"\n",
        "finetuning_dataset = load_dataset(finetuning_dataset_name)\n",
        "print(finetuning_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "a5ea65af2a4043bdbae637eef7009e0e",
            "7268f078743d4a28a2cd309185c97177",
            "628342b29d05408bb67333620c9e2961",
            "f8f970fcecab44b3a324d926a19a1d00",
            "841d3392689c4453b2a800ca6df5d80e",
            "1cb054efd5e94777ad128c4523d260c5",
            "c1fe7f3a1e394631b76480326334ecfc",
            "e7ab75e3c3084af1be800bb890480c5d",
            "d800aff5e2474522a2ab204755ec5f78",
            "1eced8007dd1480cb8afee1a5c501e57",
            "f0c8ff4dc89849ecafa2527f673699ef",
            "399d313b96b9484bb96108de05554e8b",
            "d432cffe9b9a4dd09ee6581cae0bf3e2",
            "ed9b74818a6d41528112be1927c37518",
            "bf47358c42364d1a969f2c18bd012c2e",
            "900ce173d2114f14997a2d0e4d5a364a",
            "0a33a10a23a94a119d635b10d1ecf227",
            "7cec1b785d98480aadfc06ee89b28b5f",
            "9b7cf373e19e45a39eec98615392580e",
            "ce8540f009ef4ba4ba661167df33c2d4",
            "41f5d7fde4e14584adbeb7e89e4f5747",
            "6dfe80ee3e7c4876a24ed536218e710f",
            "aa6d029de1994d1da20e5c0e75550b0b",
            "57b97368d5d542ba88b30d89169e0f32",
            "d55c29b5d73c4eb6b798fb3002291458",
            "75f19ea1202943b4bb12f9fc280978e6",
            "3d76c8fb364f47499dc5bff15657490a",
            "9803ca5b3cce435897136da2b68a5ec0",
            "ed1ee0a747114b3baf151dcfd4af122f",
            "2810bcee9683431d86071c1990e7a797",
            "2f1c0c0ce4714d2b93e17a2e029a8ba0",
            "52bf70270b0f4bab8c0541082ee60553",
            "f35da28af5ac45138d5cc945df6db0f5"
          ]
        },
        "id": "Loqrg2QBRvoz",
        "outputId": "01855029-a00d-4edc-9e08-43e3fdb4f4d0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/306 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5ea65af2a4043bdbae637eef7009e0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/25.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "399d313b96b9484bb96108de05554e8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa6d029de1994d1da20e5c0e75550b0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['question', 'answer'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIWkVWBSP9qq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}